{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5950155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3d9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and site names setup\n",
    "waves_folder_path = \"./dataset_Ondas\"\n",
    "shorelines_folder_path = \"./dataset_linhascosta\"\n",
    "transects_folder_path = \"./dataset_transects\"\n",
    "site_names = [\"CCFT\", \"NNOR\", \"MEIA\", \"TROI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0190148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store DataFrames\n",
    "data, annual_data = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6aad260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing wave and shoreline data for each site\n",
    "for name in site_names:\n",
    "    # File paths\n",
    "    waves_file_path = os.path.join(waves_folder_path, f\"{name}_wave_timeseries.csv\")\n",
    "    shorelines_file_path = os.path.join(shorelines_folder_path, f\"{name}_shoreline_timeseries.csv\")\n",
    "    transects_file_path = os.path.join(transects_folder_path, f\"{name}_transects.geojson\")\n",
    "\n",
    "    # Read the waves CSV files into DataFrame\n",
    "    waves_df = pd.read_csv(waves_file_path, sep=',', header=0) # Set header=0 to use the first row as column headers\n",
    "    waves_df['time'] = pd.to_datetime(waves_df['time'])\n",
    "    waves_df.set_index('time', inplace=True)\n",
    "    waves_df['years'] = waves_df.index.year\n",
    "    waves_df['months'] = waves_df.index.month\n",
    "    waves_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(year, month) for year, month in zip(waves_df.index.year, waves_df.index.month)],\n",
    "    names=['years', 'months'])\n",
    "    waves_df = waves_df[waves_df['years'] != 1983] # Remove 1983 because satellite data for shorelines is not available for that year\n",
    "    \n",
    "    # List of directions (16 directions compass rose)\n",
    "    directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    def degrees_to_direction(wave_direction_degrees):\n",
    "        if wave_direction_degrees >= 0 and   wave_direction_degrees <= 11.25:\n",
    "            return 'N'\n",
    "        elif wave_direction_degrees <= 33.75:\n",
    "            return 'NNE'\n",
    "        elif wave_direction_degrees <= 56.25:\n",
    "            return 'NE'\n",
    "        elif wave_direction_degrees <= 78.75:\n",
    "            return 'ENE'\n",
    "        elif wave_direction_degrees <= 101.25:\n",
    "            return 'E'\n",
    "        elif wave_direction_degrees <= 123.75:\n",
    "            return 'ESE'\n",
    "        elif wave_direction_degrees <= 146.25:\n",
    "            return 'SE'\n",
    "        elif wave_direction_degrees <= 168.75:\n",
    "            return 'SSE'\n",
    "        elif wave_direction_degrees <= 191.25:\n",
    "            return 'S'\n",
    "        elif wave_direction_degrees <= 213.75:\n",
    "            return 'SSW'\n",
    "        elif wave_direction_degrees <= 236.25:\n",
    "            return 'SW'\n",
    "        elif wave_direction_degrees <= 258.75:\n",
    "            return 'WSW'\n",
    "        elif wave_direction_degrees <= 281.25:\n",
    "            return 'W'\n",
    "        elif wave_direction_degrees <= 303.75:\n",
    "            return 'WNW'\n",
    "        elif wave_direction_degrees <= 326.25:\n",
    "            return 'NW'\n",
    "        elif wave_direction_degrees <= 348.75:\n",
    "            return 'NNW'\n",
    "        elif wave_direction_degrees <= 360:\n",
    "            return 'N'\n",
    "        else:\n",
    "            return 'false'\n",
    "  \n",
    "    # One-hot encode the 'mwd' column\n",
    "    waves_df['mwd'] = waves_df['mwd'].apply(degrees_to_direction)\n",
    "\n",
    "    # Create a DataFrame of dummy variables for 'mwd'\n",
    "    one_hot_encode = pd.get_dummies(waves_df['mwd'], prefix='from')\n",
    "\n",
    "    # Concatenate the one-hot encoded columns to the original DataFrame\n",
    "    waves_df = pd.concat([waves_df, one_hot_encode], axis=1)\n",
    "    waves_df = waves_df.drop('mwd', axis=1)\n",
    "\n",
    "    # Iterate through directions and create new columns for each direction's pp1d and swh\n",
    "    for direction in directions:\n",
    "        # Create new columns for pp1d and swh\n",
    "        pp1d_column_name = f'pp1d_from_{direction}'\n",
    "        swh_column_name = f'swh_from_{direction}'\n",
    "    \n",
    "        # Use boolean indexing to set values based on the condition\n",
    "        waves_df[pp1d_column_name] = waves_df['pp1d'] * waves_df[f'from_{direction}']\n",
    "        waves_df[swh_column_name] = waves_df['swh'] * waves_df[f'from_{direction}']\n",
    "    \n",
    "    # Drop the original 'mwd' column and the 'pp1d' and 'swh' columns\n",
    "    waves_df.drop(columns=[f'from_{direction}' for direction in directions], inplace=True)\n",
    "    waves_df.drop(columns=['pp1d','swh'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Read the shorelines CSV files into DataFrame\n",
    "    shorelines_df = pd.read_csv(shorelines_file_path)\n",
    "    shorelines_df = shorelines_df.iloc[:, 1:]\n",
    "    shorelines_df['dates'] = pd.to_datetime(shorelines_df['dates'])\n",
    "    shorelines_df.set_index('dates', inplace=True)\n",
    "    shorelines_df['years'] = shorelines_df.index.year\n",
    "    shorelines_df['months'] = shorelines_df.index.month\n",
    "    shorelines_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(year, month) for year, month in zip(shorelines_df.index.year, shorelines_df.index.month)],\n",
    "    names=['years', 'months'])\n",
    "    \n",
    "    # Add a new column to waves and shorelines dataframes to indicate the site name\n",
    "    waves_df['site'] = name\n",
    "    shorelines_df['site'] = name\n",
    "\n",
    "    # Read the transects GeoJSON file into a GeoDataFrame\n",
    "    transects_gdf = gpd.read_file(transects_file_path, driver='GeoJSON')\n",
    "\n",
    "    # Add DataFrames to the dictionary with site name as key\n",
    "    data[name] = {\n",
    "        'waves': waves_df,\n",
    "        'shorelines': shorelines_df,\n",
    "        'transects': transects_gdf\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150eca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over keys in the data dictionary\n",
    "for name in data.keys():\n",
    "    waves_df = data[name]['waves']\n",
    "    \n",
    "    # Group by 'years' and calculate quantiles for each column\n",
    "    wave_df_annual = waves_df.groupby(level=['years', 'months']).agg(\n",
    "           {\n",
    "        'pp1d_from_N': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_N': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NNE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NNE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_ENE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_ENE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_E': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_E': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_ESE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_ESE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ], \n",
    "        'swh_from_SE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SSE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SSE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_S': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_S': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_SW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_WSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_WSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_W': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_W': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_WNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_WNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ]})\n",
    "    \n",
    "\n",
    "    # Replace NaN values with zero\n",
    "    wave_df_annual = wave_df_annual.fillna(0)\n",
    "\n",
    "    shoreline_df = data[name]['shorelines']\n",
    "\n",
    "    # Group by 'years' and calculate median for each column\n",
    "    shoreline_df_annual = shoreline_df.groupby(level=['years', 'months']).median(numeric_only=True)\n",
    "    \n",
    "    # Drop year and month columns\n",
    "    shoreline_df_annual = shoreline_df_annual.drop(['years', 'months'], axis=1)\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "\n",
    "    for i in range(1, len(shoreline_df_annual.columns) - 1):\n",
    "        col = shoreline_df_annual.columns[i]\n",
    "        prev_col = shoreline_df_annual.columns[i - 1] if i - 1 >= 0 else None\n",
    "        next_col = shoreline_df_annual.columns[i + 1] if i + 1 < len(shoreline_df_annual.columns) else None\n",
    "\n",
    "        # Check if there are any NaN values in the current column\n",
    "        if shoreline_df_annual[col].isnull().any():\n",
    "            # Fill NaN values with the mean of the available previous and next columns\n",
    "            if prev_col is not None and next_col is not None:\n",
    "                shoreline_df_annual[col] = (shoreline_df_annual[prev_col] + shoreline_df_annual[next_col]) / 2\n",
    "            elif prev_col is not None:\n",
    "                shoreline_df_annual[col] = shoreline_df_annual[prev_col]\n",
    "            elif next_col is not None:\n",
    "                shoreline_df_annual[col] = shoreline_df_annual[next_col]\n",
    "            else:\n",
    "                # If there are no immediate previous and next columns, extend the search to 3 columns\n",
    "                prev_cols = [shoreline_df_annual.columns[j] for j in range(i - 2, i) if j >= 0]\n",
    "                next_cols = [shoreline_df_annual.columns[j] for j in range(i + 1, i + 4) if j < len(shoreline_df_annual.columns)]\n",
    "\n",
    "                available_cols = prev_cols + next_cols\n",
    "\n",
    "                # Filter out None values (columns that are out of range)\n",
    "                available_cols = [col for col in available_cols if col is not None]\n",
    "\n",
    "                # Take the mean of available columns\n",
    "                if len(available_cols) > 0:\n",
    "                    shoreline_df_annual[col] = shoreline_df_annual[available_cols].mean(axis=1)\n",
    "\n",
    "    \n",
    "    # Add the DataFrame to the dictionary with site name as key\n",
    "    annual_data[name] = {\n",
    "        'waves': wave_df_annual,\n",
    "        'shorelines': shoreline_df_annual\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda8c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79c0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ddf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69218edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4021fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>TROI_1</th>\n",
       "      <th>TROI_2</th>\n",
       "      <th>TROI_3</th>\n",
       "      <th>TROI_4</th>\n",
       "      <th>TROI_5</th>\n",
       "      <th>TROI_6</th>\n",
       "      <th>TROI_7</th>\n",
       "      <th>TROI_8</th>\n",
       "      <th>TROI_9</th>\n",
       "      <th>TROI_10</th>\n",
       "      <th>...</th>\n",
       "      <th>TROI_13</th>\n",
       "      <th>TROI_14</th>\n",
       "      <th>TROI_15</th>\n",
       "      <th>TROI_16</th>\n",
       "      <th>TROI_17</th>\n",
       "      <th>TROI_18</th>\n",
       "      <th>TROI_20</th>\n",
       "      <th>TROI_21</th>\n",
       "      <th>TROI_22</th>\n",
       "      <th>TROI_23</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <th>months</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TROI_1, TROI_2, TROI_3, TROI_4, TROI_5, TROI_6, TROI_7, TROI_8, TROI_9, TROI_10, TROI_11, TROI_12, TROI_13, TROI_14, TROI_15, TROI_16, TROI_17, TROI_18, TROI_20, TROI_21, TROI_22, TROI_23]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annual_data['TROI']['shorelines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine standardized data into a single empty DataFrame with all months and years from the previous tables\n",
    "combined_shorelines = pd.DataFrame(index=pd.MultiIndex.from_product(\n",
    "    [shoreline_df.index.get_level_values(0).unique(), shoreline_df.index.get_level_values(1).unique()],\n",
    "    names=['years', 'months']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from all sites - SHORELINES\n",
    "for name in site_names:\n",
    "    df_to_merge = annual_data[name]['shorelines']\n",
    "\n",
    "    # Reset index if 'years' and 'months' are part of the index\n",
    "    if 'years' in df_to_merge.index.names and 'months' in df_to_merge.index.names:\n",
    "        df_to_merge = df_to_merge.reset_index()\n",
    "\n",
    "    combined_shorelines = combined_shorelines.merge(df_to_merge, \n",
    "                                                   on=['years', 'months'], \n",
    "                                                   how='left')\n",
    "\n",
    "# Handling NaNs - Group by 'years' and fill NaNs with the mean of the respective year\n",
    "for column in combined_shorelines.columns:\n",
    "    if column not in ['site', 'years', 'months']:\n",
    "        combined_shorelines[column] = combined_shorelines.groupby('years')[column].transform(lambda x: x.fillna(x.mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de126946",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_df_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure that the waves dataframe has the same dimensions as the shorelines\n",
    "\n",
    "combined_waves = pd.DataFrame(index=pd.MultiIndex.from_product(\n",
    "    [shoreline_df.index.get_level_values(0).unique(), shoreline_df.index.get_level_values(1).unique()],\n",
    "    names=['years', 'months']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from all sites - SHORELINES\n",
    "\n",
    "df_to_merge_waves = wave_df_annual\n",
    "\n",
    "# Reset index if 'years' and 'months' are part of the index\n",
    "if 'years' in df_to_merge_waves.index and 'months' in df_to_merge.index_waves:\n",
    "    df_to_merge_waves = df_to_merge_waves.reset_index()\n",
    "\n",
    "combined_waves = combined_waves.merge(df_to_merge_waves, \n",
    "                                      on=['years', 'months'], \n",
    "                                      how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9371e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combined_waves.shape\n",
    "combined_shorelines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = combined_waves  # Assuming 'combined_waves' contains your features\n",
    "y = combined_shorelines  # Assuming 'combined_shorelines' contains your target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ddb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b378d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassigning multi-index to combined_shorelines table\n",
    "#combined_shorelines = combined_shorelines.set_index(['years', 'months'])\n",
    "combined_shorelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "\n",
    "# Create training datasets\n",
    "x_train = combined_waves[combined_waves.index.get_level_values(0) <= 2014]\n",
    "y_train = combined_shorelines[combined_shorelines.index.get_level_values(0) <= 2014]\n",
    "\n",
    "# Create testing datasets\n",
    "x_test = combined_waves[combined_waves.index.get_level_values(0) > 2014]\n",
    "y_test = combined_shorelines[combined_shorelines.index.get_level_values(0) > 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d35b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeRegressor model\n",
    "model = DecisionTreeRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eceeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict shoreline positions\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=y_test.columns, index=y_test.index)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94fd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02c13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe1bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c4fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2affb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0faffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50615490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c186a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee59191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b456d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    for column in shoreline_df_annual.columns:\n",
    "        # Check if there are any NaN values in the column\n",
    "        if shoreline_df_annual[column].isnull().any():\n",
    "            \n",
    "            # Final check and fill/drop remaining NaNs\n",
    "            shoreline_df_annual.fillna(method='ffill', inplace=True)\n",
    "            shoreline_df_annual.fillna(method='bfill', inplace=True)\n",
    "            \n",
    "            # Calculate the median value of the column (excluding NaN values)\n",
    "            median_value = shoreline_df_annual[column].median()\n",
    "        \n",
    "            # Replace NaN values with the calculated median value\n",
    "            shoreline_df_annual[column].fillna(median_value, inplace=True)\n",
    "\n",
    "            # Ensure no NaNs are left before model training\n",
    "            if shoreline_df_annual.isna().any().any():\n",
    "                print(f\"NaNs remain in shorelines data for {name}\")\n",
    "                continue  # Skip this iteration if NaNs are still present"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
