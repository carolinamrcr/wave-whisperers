{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda94414",
   "metadata": {},
   "source": [
    "# Capstone Project - Wave whisperers (Group 6)\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "    1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071398c3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2963acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7600270",
   "metadata": {},
   "source": [
    "# Data collection and preprocessing\n",
    "\n",
    "Data is retrieved after the previous extraction on waves and shorelines for the 20 selected sites:\n",
    "- Shoreline data was retrieved using CoastSat, an open-source software toolkit that enables users to obtain time-series of shoreline position at any coastal stretch from publicly available satellite imagery.\n",
    "- Wave data was extracted from the ERA5 programme dataset for meteorological and global climate reanalysis, available at the European Centre for Medium-Range Weather Forecasts (ECMWF) website.\n",
    "\n",
    "After loading the data from the csv's, wave and shoreline data are manipulated with a loop through each site name, structuring waves_df and shorelines_df into a combined dictionary.\n",
    "Regarding waves, a deeper preprocessing is required in order to convert degrees into directions and considering both height and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "76ad5647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and site names setup\n",
    "waves_folder_path = \"./dataset_Ondas\"\n",
    "shorelines_folder_path = \"./dataset_linhascosta\"\n",
    "transects_folder_path = \"./dataset_transects\"\n",
    "site_names = ['CVCC','CCFT','FTAD','ADLA','LABI',\n",
    "              'TRAT','ATMC','MCCO','CCCL','NNOR',\n",
    "              'MEIA','TORR','CVMR','MRMG','MGVR',\n",
    "              'COSN','VAGR','GBHA','BARR','MIRA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "58f7d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store DataFrames\n",
    "data = {}\n",
    "\n",
    "# Loop through each file name\n",
    "for name in site_names:\n",
    "    # Construct the file paths\n",
    "    waves_file_path = os.path.join(waves_folder_path, f\"{name}_wave_timeseries.csv\")\n",
    "    shorelines_file_path = os.path.join(shorelines_folder_path, f\"{name}_shoreline_timeseries.csv\")\n",
    "    transects_file_path = os.path.join(transects_folder_path, f\"{name}_T.geojson\")\n",
    "\n",
    "    # Read the waves CSV files into DataFrame\n",
    "    waves_df = pd.read_csv(waves_file_path, sep=',', header=0) # Set header=0 to use the first row as column headers\n",
    "    \n",
    "    waves_df['time'] = pd.to_datetime(waves_df['time'])\n",
    "    waves_df.set_index('time', inplace=True)\n",
    "\n",
    "    # Assuming 'time' column is already in datetime format and set as the index\n",
    "    waves_df['years'] = waves_df.index.year\n",
    "    waves_df['months'] = waves_df.index.month\n",
    "\n",
    "    # Remove rows from January 1983 to March 1989\n",
    "    waves_df = waves_df[~((waves_df.index >= '1983-01-01') & (waves_df.index <= '1989-03-31'))]\n",
    "\n",
    "    # If you want to keep 'time' as the index and recreate MultiIndex\n",
    "    waves_df.index = pd.MultiIndex.from_tuples(\n",
    "        [(year, month) for year, month in zip(waves_df.index.year, waves_df.index.month)],\n",
    "        names=['years', 'months'])\n",
    "    \n",
    "    # List of directions (16 directions compass rose)\n",
    "    directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    def degrees_to_direction(wave_direction_degrees):\n",
    "        if wave_direction_degrees >= 0 and   wave_direction_degrees <= 11.25:\n",
    "            return 'N'\n",
    "        elif wave_direction_degrees <= 33.75:\n",
    "            return 'NNE'\n",
    "        elif wave_direction_degrees <= 56.25:\n",
    "            return 'NE'\n",
    "        elif wave_direction_degrees <= 78.75:\n",
    "            return 'ENE'\n",
    "        elif wave_direction_degrees <= 101.25:\n",
    "            return 'E'\n",
    "        elif wave_direction_degrees <= 123.75:\n",
    "            return 'ESE'\n",
    "        elif wave_direction_degrees <= 146.25:\n",
    "            return 'SE'\n",
    "        elif wave_direction_degrees <= 168.75:\n",
    "            return 'SSE'\n",
    "        elif wave_direction_degrees <= 191.25:\n",
    "            return 'S'\n",
    "        elif wave_direction_degrees <= 213.75:\n",
    "            return 'SSW'\n",
    "        elif wave_direction_degrees <= 236.25:\n",
    "            return 'SW'\n",
    "        elif wave_direction_degrees <= 258.75:\n",
    "            return 'WSW'\n",
    "        elif wave_direction_degrees <= 281.25:\n",
    "            return 'W'\n",
    "        elif wave_direction_degrees <= 303.75:\n",
    "            return 'WNW'\n",
    "        elif wave_direction_degrees <= 326.25:\n",
    "            return 'NW'\n",
    "        elif wave_direction_degrees <= 348.75:\n",
    "            return 'NNW'\n",
    "        elif wave_direction_degrees <= 360:\n",
    "            return 'N'\n",
    "        else:\n",
    "            return 'false'\n",
    "\n",
    "    # One-hot encode the 'mwd' column\n",
    "    waves_df['mwd'] = waves_df['mwd'].apply(degrees_to_direction)\n",
    "\n",
    "    # Create a DataFrame of dummy variables for 'mwd'\n",
    "    one_hot_encode = pd.get_dummies(waves_df['mwd'], prefix='from')\n",
    "\n",
    "    # Concatenate the one-hot encoded columns to the original DataFrame\n",
    "    waves_df = pd.concat([waves_df, one_hot_encode], axis=1)\n",
    "    waves_df = waves_df.drop('mwd', axis=1)\n",
    "\n",
    "    # Iterate through directions and create new columns for each direction's pp1d and swh\n",
    "    for direction in directions:\n",
    "        # Create new columns for pp1d and swh\n",
    "        pp1d_column_name = f'{name}_pp1d_from_{direction}'\n",
    "        swh_column_name = f'{name}_swh_from_{direction}'\n",
    "    \n",
    "        # Use boolean indexing to set values based on the condition\n",
    "        waves_df[pp1d_column_name] = waves_df['pp1d'] * waves_df[f'from_{direction}']\n",
    "        waves_df[swh_column_name] = waves_df['swh'] * waves_df[f'from_{direction}']\n",
    "    \n",
    "    # Drop the original 'mwd' column and the 'pp1d' and 'swh' columns\n",
    "    waves_df.drop(columns=[f'from_{direction}' for direction in directions], inplace=True)\n",
    "    waves_df.drop(columns=['pp1d','swh'], inplace=True)\n",
    "\n",
    "    # Read the shorelines CSV files into DataFrame\n",
    "    shorelines_df = pd.read_csv(shorelines_file_path)\n",
    "    shorelines_df = shorelines_df.iloc[:, 1:]\n",
    "    shorelines_df['dates'] = pd.to_datetime(shorelines_df['dates'])\n",
    "    shorelines_df.set_index('dates', inplace=True)\n",
    "\n",
    "    # Assuming 'time' column is already in datetime format and set as the index\n",
    "    shorelines_df['years'] = shorelines_df.index.year\n",
    "    shorelines_df['months'] = shorelines_df.index.month\n",
    "\n",
    "    # Remove rows from January 1983 to March 1989\n",
    "    shorelines_df = shorelines_df[~((shorelines_df.index >= '1983-01-01') & (shorelines_df.index <= '1989-03-31'))]\n",
    "\n",
    "    # Recreate MultiIndex\n",
    "    shorelines_df.index = pd.MultiIndex.from_tuples(\n",
    "        [(year, month) for year, month in zip(shorelines_df.index.year, shorelines_df.index.month)],\n",
    "        names=['years', 'months'])\n",
    "\n",
    "    # Read the transects GeoJSON file into a GeoDataFrame\n",
    "    transects_gdf = gpd.read_file(transects_file_path, driver='GeoJSON')\n",
    "    transects_gdf['beach'] = transects_gdf['name'].apply(lambda x: x.split('_')[0] if 'MIRA' in x else None)\n",
    "\n",
    "    # Create a 'transect' column\n",
    "    transects_gdf['transect'] = transects_gdf['name'].apply(lambda x: f'P{x.split(\"_\")[1]}' if 'MIRA' in x else None)\n",
    "\n",
    "    # Drop the 'name' column\n",
    "    transects_gdf.drop(columns=['name'], inplace=True)\n",
    "\n",
    "    # Add DataFrames to the dictionary with site name as key\n",
    "    data[name] = {\n",
    "        'waves': waves_df,\n",
    "        'shorelines': shorelines_df,\n",
    "        'transects': transects_gdf\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845c5c2",
   "metadata": {},
   "source": [
    "# Data Aggregation and Missing Data Handling\n",
    "\n",
    "As seen in the Exploratory Data Analysis, regarding shoreline data (available since 1984 for all sites), there is no defined periodicity observed, since there could be many observations in the same month or none.\n",
    "Regarding wave data, periodicity is hourly (and the availability is since before 1984, but it was only considered since then).\n",
    "In order to define the same periodicity for both datasets, it was defined that the data would be aggregated by year and month.\n",
    "To do this aggregation, median values by year and month were considered.\n",
    "\n",
    "Regarding NaN handling:\n",
    "- Wave data: NaNs were filled with 0's. This is because NaNs occur when creating the columns for heigh and period, when they're not specified. And in this case we considered it to be 0.\n",
    "- Shoreline data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ed1617f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1217: RuntimeWarning: All-NaN slice encountered\n",
      "  r, k = function_base._ureduce(a, func=_nanmedian, axis=axis, out=out,\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to store the results\n",
    "annual_data = {}\n",
    "\n",
    "# Iterate over keys in the data dictionary\n",
    "for name in data.keys():\n",
    "    waves_df = data[name]['waves']\n",
    "\n",
    "    waves_df = waves_df.drop(['years', 'months'], axis=1)\n",
    "    \n",
    "    waves_df_annual = waves_df.groupby([waves_df.index.get_level_values('years'), waves_df.index.get_level_values('months')]).agg(\n",
    "           {\n",
    "        f'{name}_pp1d_from_N'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_N'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_NNE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_NNE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_NE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_NE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_ENE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_ENE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_E'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_E'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_ESE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_ESE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_SE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_SE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_SSE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_SSE'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_S'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_S'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_SSW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_SSW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_SW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_SW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_WSW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_WSW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_W'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_W'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_WNW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_WNW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_NW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_NW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_pp1d_from_NNW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None,\n",
    "        f'{name}_swh_from_NNW'  : lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None\n",
    "        })\n",
    "\n",
    "    waves_df_annual = waves_df_annual.fillna(0)\n",
    "    \n",
    "    shoreline_df = data[name]['shorelines']\n",
    "\n",
    "    all_years = range(1984, 2023)\n",
    "    all_months = range(1, 13)\n",
    "\n",
    "    # Create all combinations starting from April 1984\n",
    "    all_combinations = [(year, month) for year in all_years for month in all_months]\n",
    "\n",
    "    # Filter combinations to start from April\n",
    "    all_combinations = [(year, month) for year, month in all_combinations if (year, month) >= (1984, 4)]\n",
    "\n",
    "    # Create MultiIndex\n",
    "    multi_index = pd.MultiIndex.from_tuples(all_combinations, names=['years', 'months'])\n",
    "\n",
    "    # Group by the MultiIndex and calculate the median\n",
    "    shoreline_df_annual = shoreline_df.groupby(level=['years', 'months']).median(numeric_only=True)\n",
    "\n",
    "    # Reindex with the full MultiIndex to fill missing combinations with NaN\n",
    "    shoreline_df_annual = shoreline_df_annual.reindex(multi_index)\n",
    "    \n",
    "    # Check for columns that only have NaN values after reindexing\n",
    "    empty_columns_after_reindexing = shoreline_df_annual.columns[shoreline_df_annual.isnull().all()]\n",
    "\n",
    "   # Drop year and month columns\n",
    "    shoreline_df_annual = shoreline_df_annual.drop(['years', 'months'], axis=1)\n",
    "\n",
    "        # Iterate over each column in the DataFrame \n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for i in range(1, len(shoreline_df_annual.columns) - 1):\n",
    "        col = shoreline_df_annual.columns[i]\n",
    "\n",
    "        # Skip columns with names \"years\" or \"months\"\n",
    "        if col.lower() not in ['years', 'months']:\n",
    "            for idx in shoreline_df_annual[col][shoreline_df_annual[col].isnull()].index:\n",
    "                prev_col = shoreline_df_annual.columns[i - 1] if i - 1 >= 0 else None\n",
    "                next_col = shoreline_df_annual.columns[i + 1] if i + 1 < len(shoreline_df_annual.columns) else None\n",
    "\n",
    "                # Check if adjacent columns have non-NaN values and use them for filling NaNs\n",
    "                if prev_col and next_col:\n",
    "                    prev_val = shoreline_df_annual.at[idx, prev_col]\n",
    "                    next_val = shoreline_df_annual.at[idx, next_col]\n",
    "                    if pd.notnull(prev_val) and pd.notnull(next_val):\n",
    "                        shoreline_df_annual.at[idx, col] = (prev_val + next_val) / 2\n",
    "                    elif pd.notnull(prev_val):\n",
    "                        shoreline_df_annual.at[idx, col] = prev_val\n",
    "                    elif pd.notnull(next_val):\n",
    "                        shoreline_df_annual.at[idx, col] = next_val\n",
    "                elif prev_col:\n",
    "                    prev_val = shoreline_df_annual.at[idx, prev_col]\n",
    "                    if pd.notnull(prev_val):\n",
    "                        shoreline_df_annual.at[idx, col] = prev_val\n",
    "                elif next_col:\n",
    "                    next_val = shoreline_df_annual.at[idx, next_col]\n",
    "                    if pd.notnull(next_val):\n",
    "                        shoreline_df_annual.at[idx, col] = next_val\n",
    "\n",
    "    # Perform median replacement for each row\n",
    "    for index, row in shoreline_df_annual.iterrows():\n",
    "        # Exclude \"years\" and \"months\" from median calculation\n",
    "        relevant_values = [value for column, value in row.items() if column.lower() not in ['years', 'months']]\n",
    "    \n",
    "        # Check if there are any NaN values in the relevant values\n",
    "        if any(pd.isnull(val) for val in relevant_values):\n",
    "            # Calculate the median value of the relevant values (excluding NaN values)\n",
    "            median_value = np.nanmedian(relevant_values)\n",
    "        \n",
    "            # Replace NaN values in the row with the calculated median value\n",
    "            shoreline_df_annual.loc[index, shoreline_df_annual.columns.difference(['years', 'months'])] = \\\n",
    "                shoreline_df_annual.loc[index, shoreline_df_annual.columns.difference(['years', 'months'])].fillna(median_value)\n",
    "\n",
    "    # Replace remaining NaN values in each column with the median of that column\n",
    "    shoreline_df_annual.fillna(shoreline_df_annual.median(), inplace=True)\n",
    "\n",
    "    # Ensure no NaNs are left before model training\n",
    "    if shoreline_df_annual.isna().any().any():\n",
    "        print(f\"NaNs remain in shorelines data for {name}\")\n",
    "        continue  # Skip this iteration if NaNs are still present\n",
    "\n",
    "    # Add the DataFrame to the dictionary with site name as key\n",
    "    annual_data[name] = {\n",
    "        'waves': waves_df_annual,\n",
    "        'shorelines': shoreline_df_annual,\n",
    "        'transects': transects_gdf\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_data['TORR']['shorelines']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b4ee31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all wave dataframes\n",
    "waves_dfs = []\n",
    "\n",
    "# Loop through each beach's data\n",
    "for name in annual_data:\n",
    "    # Copy the wave DataFrame\n",
    "    df = annual_data[name]['waves'].copy()\n",
    "\n",
    "    # Reset index to turn MultiIndex into columns\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Rename columns to remove the beach prefix\n",
    "    new_columns = {col: col.replace(f'{name}_', '') for col in df.columns}\n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "    # Add a column for the beach name\n",
    "    df['beach'] = name\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]  # move 'beach' to the front\n",
    "    df = df[cols]\n",
    "\n",
    "    # Append to the list\n",
    "    waves_dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "waves_combined = pd.concat(waves_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b17aaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all shoreline dataframes\n",
    "shorelines_dfs = []\n",
    "\n",
    "# Loop through each beach's data\n",
    "for name in annual_data:\n",
    "    # Copy the shoreline DataFrame\n",
    "    df = annual_data[name]['shorelines'].copy()\n",
    "\n",
    "    # Reset index to turn MultiIndex into columns\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Rename columns to use generic names\n",
    "    new_columns = {col: f'{col.split(\"_\")[-1]}' for col in df.columns}\n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "\n",
    "    # Add a column for the beach name\n",
    "    df['beach'] = name\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]  # move 'beach' to the front\n",
    "    df = df[cols]\n",
    "\n",
    "    # Append to the list\n",
    "    shorelines_dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "shorelines_combined = pd.concat(shorelines_dfs, ignore_index=True)\n",
    "# Assuming shorelines_combined is your DataFrame\n",
    "shorelines_combined.columns = shorelines_combined.columns[:3].tolist() + [f'P{i}' for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "66987d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all transect dataframes\n",
    "transects_dfs = []\n",
    "\n",
    "# Loop through each beach's data\n",
    "for name in annual_data:\n",
    "    # Copy the transect DataFrame\n",
    "    df = annual_data[name]['transects'].copy()\n",
    "\n",
    "    # Add a column for the beach name\n",
    "    df['beach'] = name\n",
    "\n",
    "    # Reorder columns\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-2:] + cols[:-2]  # move 'beach' and 'transect' to the front\n",
    "    df = df[cols]\n",
    "\n",
    "    # Append to the list\n",
    "    transects_dfs.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "transects_combined = pd.concat(transects_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "48e74f7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beach</th>\n",
       "      <th>years</th>\n",
       "      <th>months</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>TORR</td>\n",
       "      <td>1984</td>\n",
       "      <td>4</td>\n",
       "      <td>131.188227</td>\n",
       "      <td>65.893988</td>\n",
       "      <td>72.467140</td>\n",
       "      <td>116.605018</td>\n",
       "      <td>195.822139</td>\n",
       "      <td>204.391650</td>\n",
       "      <td>226.834386</td>\n",
       "      <td>390.283933</td>\n",
       "      <td>293.132171</td>\n",
       "      <td>210.863854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>TORR</td>\n",
       "      <td>1984</td>\n",
       "      <td>5</td>\n",
       "      <td>131.188227</td>\n",
       "      <td>65.893988</td>\n",
       "      <td>72.467140</td>\n",
       "      <td>116.605018</td>\n",
       "      <td>195.822139</td>\n",
       "      <td>204.391650</td>\n",
       "      <td>226.834386</td>\n",
       "      <td>390.283933</td>\n",
       "      <td>293.132171</td>\n",
       "      <td>210.863854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>TORR</td>\n",
       "      <td>1984</td>\n",
       "      <td>6</td>\n",
       "      <td>131.188227</td>\n",
       "      <td>65.893988</td>\n",
       "      <td>72.467140</td>\n",
       "      <td>116.605018</td>\n",
       "      <td>195.822139</td>\n",
       "      <td>204.391650</td>\n",
       "      <td>226.834386</td>\n",
       "      <td>390.283933</td>\n",
       "      <td>293.132171</td>\n",
       "      <td>210.863854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>TORR</td>\n",
       "      <td>1984</td>\n",
       "      <td>7</td>\n",
       "      <td>131.188227</td>\n",
       "      <td>65.893988</td>\n",
       "      <td>72.467140</td>\n",
       "      <td>116.605018</td>\n",
       "      <td>195.822139</td>\n",
       "      <td>204.391650</td>\n",
       "      <td>226.834386</td>\n",
       "      <td>390.283933</td>\n",
       "      <td>293.132171</td>\n",
       "      <td>210.863854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>TORR</td>\n",
       "      <td>1984</td>\n",
       "      <td>8</td>\n",
       "      <td>131.188227</td>\n",
       "      <td>65.893988</td>\n",
       "      <td>72.467140</td>\n",
       "      <td>116.605018</td>\n",
       "      <td>195.822139</td>\n",
       "      <td>204.391650</td>\n",
       "      <td>226.834386</td>\n",
       "      <td>390.283933</td>\n",
       "      <td>293.132171</td>\n",
       "      <td>210.863854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5575</th>\n",
       "      <td>TORR</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>181.887574</td>\n",
       "      <td>83.798995</td>\n",
       "      <td>77.919675</td>\n",
       "      <td>126.238288</td>\n",
       "      <td>196.958431</td>\n",
       "      <td>219.431143</td>\n",
       "      <td>236.212974</td>\n",
       "      <td>407.935886</td>\n",
       "      <td>340.655383</td>\n",
       "      <td>215.041576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5576</th>\n",
       "      <td>TORR</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>180.021687</td>\n",
       "      <td>84.210283</td>\n",
       "      <td>81.297094</td>\n",
       "      <td>129.860962</td>\n",
       "      <td>201.877167</td>\n",
       "      <td>221.104556</td>\n",
       "      <td>249.945333</td>\n",
       "      <td>408.507229</td>\n",
       "      <td>330.339002</td>\n",
       "      <td>215.707524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5577</th>\n",
       "      <td>TORR</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>172.687428</td>\n",
       "      <td>86.963107</td>\n",
       "      <td>82.404846</td>\n",
       "      <td>132.245063</td>\n",
       "      <td>204.154545</td>\n",
       "      <td>226.140038</td>\n",
       "      <td>251.290478</td>\n",
       "      <td>391.995482</td>\n",
       "      <td>296.135846</td>\n",
       "      <td>215.890059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5578</th>\n",
       "      <td>TORR</td>\n",
       "      <td>2022</td>\n",
       "      <td>11</td>\n",
       "      <td>173.316393</td>\n",
       "      <td>79.779173</td>\n",
       "      <td>80.172242</td>\n",
       "      <td>120.359394</td>\n",
       "      <td>197.344095</td>\n",
       "      <td>211.940968</td>\n",
       "      <td>246.774350</td>\n",
       "      <td>392.602328</td>\n",
       "      <td>296.745704</td>\n",
       "      <td>204.052931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5579</th>\n",
       "      <td>TORR</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>141.829550</td>\n",
       "      <td>71.736926</td>\n",
       "      <td>70.085427</td>\n",
       "      <td>113.338113</td>\n",
       "      <td>189.496443</td>\n",
       "      <td>199.457001</td>\n",
       "      <td>243.940263</td>\n",
       "      <td>385.907255</td>\n",
       "      <td>255.913321</td>\n",
       "      <td>195.863656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>465 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     beach  years  months          P1         P2         P3          P4  \\\n",
       "5115  TORR   1984       4  131.188227  65.893988  72.467140  116.605018   \n",
       "5116  TORR   1984       5  131.188227  65.893988  72.467140  116.605018   \n",
       "5117  TORR   1984       6  131.188227  65.893988  72.467140  116.605018   \n",
       "5118  TORR   1984       7  131.188227  65.893988  72.467140  116.605018   \n",
       "5119  TORR   1984       8  131.188227  65.893988  72.467140  116.605018   \n",
       "...    ...    ...     ...         ...        ...        ...         ...   \n",
       "5575  TORR   2022       8  181.887574  83.798995  77.919675  126.238288   \n",
       "5576  TORR   2022       9  180.021687  84.210283  81.297094  129.860962   \n",
       "5577  TORR   2022      10  172.687428  86.963107  82.404846  132.245063   \n",
       "5578  TORR   2022      11  173.316393  79.779173  80.172242  120.359394   \n",
       "5579  TORR   2022      12  141.829550  71.736926  70.085427  113.338113   \n",
       "\n",
       "              P5          P6          P7          P8          P9         P10  \n",
       "5115  195.822139  204.391650  226.834386  390.283933  293.132171  210.863854  \n",
       "5116  195.822139  204.391650  226.834386  390.283933  293.132171  210.863854  \n",
       "5117  195.822139  204.391650  226.834386  390.283933  293.132171  210.863854  \n",
       "5118  195.822139  204.391650  226.834386  390.283933  293.132171  210.863854  \n",
       "5119  195.822139  204.391650  226.834386  390.283933  293.132171  210.863854  \n",
       "...          ...         ...         ...         ...         ...         ...  \n",
       "5575  196.958431  219.431143  236.212974  407.935886  340.655383  215.041576  \n",
       "5576  201.877167  221.104556  249.945333  408.507229  330.339002  215.707524  \n",
       "5577  204.154545  226.140038  251.290478  391.995482  296.135846  215.890059  \n",
       "5578  197.344095  211.940968  246.774350  392.602328  296.745704  204.052931  \n",
       "5579  189.496443  199.457001  243.940263  385.907255  255.913321  195.863656  \n",
       "\n",
       "[465 rows x 13 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torr_shorelines = shorelines_combined[shorelines_combined['beach'] == 'TORR']\n",
    "torr_shorelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f86bde69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization function\n",
    "def normalize_by_reference(group):\n",
    "    # Select the first row as the reference row\n",
    "    reference_row = group.iloc[0]\n",
    "\n",
    "    # Normalize each column by subtracting the corresponding reference value\n",
    "    for col in group.columns:\n",
    "        if col not in ['years', 'months', 'beach']:\n",
    "            if not np.isnan(reference_row[col]):\n",
    "                group[col] -= reference_row[col]\n",
    "\n",
    "    return group\n",
    "\n",
    "# Apply the normalization function to each beach group\n",
    "shorelines_normalized = shorelines_combined.groupby('beach', group_keys=True).apply(normalize_by_reference).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b340f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torr_shorelines_normalized = shorelines_normalized[shorelines_normalized['beach'] == 'TORR']\n",
    "torr_shorelines_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1925477",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the waves_combined and shorelines_normalized dataframes\n",
    "# This will join the tables on 'years', 'months', and 'beach'\n",
    "combined_data_1 = pd.merge(waves_combined, shorelines_normalized, how='inner', on=['years', 'months', 'beach'])\n",
    "combined_data_1 = combined_data_1.sort_values(by=['years', 'months']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0c1a424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the 'beach' column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "beach_encoded = encoder.fit_transform(combined_data_1[['beach']])\n",
    "\n",
    "# Create a DataFrame from the encoded array\n",
    "beach_encoded_df = pd.DataFrame(beach_encoded, columns=encoder.get_feature_names_out(['beach']), index=combined_data_1.index)\n",
    "\n",
    "# Drop the original 'beach' column from combined_data\n",
    "combined_data = combined_data_1.drop('beach', axis=1)\n",
    "\n",
    "# Concatenate the one-hot encoded beach column with combined_data\n",
    "combined_data = pd.concat([combined_data, beach_encoded_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "58eba44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the combined data into training and testing sets\n",
    "\n",
    "# Filter rows for training data (years <= 2014)\n",
    "training_data = combined_data[combined_data['years'] <= 2014]\n",
    "\n",
    "# Filter rows for testing data (years > 2014)\n",
    "testing_data = combined_data[combined_data['years'] > 2014]\n",
    "\n",
    "# Define the columns for the target variables\n",
    "target_columns = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10']\n",
    "\n",
    "# For training set\n",
    "x_train = training_data.drop(target_columns + ['years', 'months'], axis=1)\n",
    "y_train = training_data[target_columns]\n",
    "\n",
    "# For testing set\n",
    "x_test = testing_data.drop(target_columns + ['years', 'months'], axis=1)\n",
    "y_test = testing_data[target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c26a2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeRegressor model\n",
    "model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d4811322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7a13813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P7</th>\n",
       "      <th>P8</th>\n",
       "      <th>P9</th>\n",
       "      <th>P10</th>\n",
       "      <th>beach</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>81.230095</td>\n",
       "      <td>0.018819</td>\n",
       "      <td>0.238840</td>\n",
       "      <td>7.845328</td>\n",
       "      <td>10.371871</td>\n",
       "      <td>19.672464</td>\n",
       "      <td>14.884869</td>\n",
       "      <td>30.097223</td>\n",
       "      <td>-18.873412</td>\n",
       "      <td>8.675822</td>\n",
       "      <td>CVCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6201</th>\n",
       "      <td>14.562370</td>\n",
       "      <td>14.225984</td>\n",
       "      <td>34.098479</td>\n",
       "      <td>3.510176</td>\n",
       "      <td>13.887059</td>\n",
       "      <td>11.804680</td>\n",
       "      <td>-9.443531</td>\n",
       "      <td>-9.245408</td>\n",
       "      <td>-9.439674</td>\n",
       "      <td>12.999496</td>\n",
       "      <td>CCFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6202</th>\n",
       "      <td>15.367606</td>\n",
       "      <td>11.050692</td>\n",
       "      <td>14.720085</td>\n",
       "      <td>4.063437</td>\n",
       "      <td>9.668787</td>\n",
       "      <td>-1.218458</td>\n",
       "      <td>9.790987</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>-10.467990</td>\n",
       "      <td>0.481180</td>\n",
       "      <td>FTAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6203</th>\n",
       "      <td>3.490658</td>\n",
       "      <td>3.194526</td>\n",
       "      <td>5.930515</td>\n",
       "      <td>4.109576</td>\n",
       "      <td>5.209023</td>\n",
       "      <td>5.515990</td>\n",
       "      <td>1.536982</td>\n",
       "      <td>0.937429</td>\n",
       "      <td>-3.339971</td>\n",
       "      <td>7.273731</td>\n",
       "      <td>ADLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6204</th>\n",
       "      <td>11.812888</td>\n",
       "      <td>6.794182</td>\n",
       "      <td>16.405575</td>\n",
       "      <td>-4.459362</td>\n",
       "      <td>-3.393964</td>\n",
       "      <td>-9.162853</td>\n",
       "      <td>-8.991966</td>\n",
       "      <td>-0.814171</td>\n",
       "      <td>-20.100475</td>\n",
       "      <td>-11.294160</td>\n",
       "      <td>LABI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8115</th>\n",
       "      <td>-43.929247</td>\n",
       "      <td>-27.726410</td>\n",
       "      <td>-79.841180</td>\n",
       "      <td>-63.196436</td>\n",
       "      <td>-42.732187</td>\n",
       "      <td>-48.227991</td>\n",
       "      <td>1.909535</td>\n",
       "      <td>-24.424192</td>\n",
       "      <td>1.786017</td>\n",
       "      <td>-28.490322</td>\n",
       "      <td>COSN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8116</th>\n",
       "      <td>-20.810362</td>\n",
       "      <td>-22.059707</td>\n",
       "      <td>-17.869315</td>\n",
       "      <td>-21.630702</td>\n",
       "      <td>-26.578442</td>\n",
       "      <td>-20.143766</td>\n",
       "      <td>-23.981757</td>\n",
       "      <td>-18.589730</td>\n",
       "      <td>-32.415639</td>\n",
       "      <td>-20.811116</td>\n",
       "      <td>VAGR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8117</th>\n",
       "      <td>-26.124367</td>\n",
       "      <td>-55.816301</td>\n",
       "      <td>-37.078218</td>\n",
       "      <td>-25.284482</td>\n",
       "      <td>0.382884</td>\n",
       "      <td>-14.342723</td>\n",
       "      <td>-16.855074</td>\n",
       "      <td>3.686623</td>\n",
       "      <td>-29.390967</td>\n",
       "      <td>-38.576515</td>\n",
       "      <td>GBHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8118</th>\n",
       "      <td>-42.209128</td>\n",
       "      <td>-27.724533</td>\n",
       "      <td>-28.955970</td>\n",
       "      <td>-9.479079</td>\n",
       "      <td>-11.715716</td>\n",
       "      <td>-70.514130</td>\n",
       "      <td>-48.037738</td>\n",
       "      <td>-49.362695</td>\n",
       "      <td>-53.586364</td>\n",
       "      <td>-36.346653</td>\n",
       "      <td>BARR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8119</th>\n",
       "      <td>-42.256740</td>\n",
       "      <td>-33.737129</td>\n",
       "      <td>-10.603109</td>\n",
       "      <td>-26.980900</td>\n",
       "      <td>0.444167</td>\n",
       "      <td>-17.125913</td>\n",
       "      <td>-48.509492</td>\n",
       "      <td>-63.286528</td>\n",
       "      <td>-62.312174</td>\n",
       "      <td>-35.148936</td>\n",
       "      <td>MIRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1920 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             P1         P2         P3         P4         P5         P6  \\\n",
       "6200  81.230095   0.018819   0.238840   7.845328  10.371871  19.672464   \n",
       "6201  14.562370  14.225984  34.098479   3.510176  13.887059  11.804680   \n",
       "6202  15.367606  11.050692  14.720085   4.063437   9.668787  -1.218458   \n",
       "6203   3.490658   3.194526   5.930515   4.109576   5.209023   5.515990   \n",
       "6204  11.812888   6.794182  16.405575  -4.459362  -3.393964  -9.162853   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "8115 -43.929247 -27.726410 -79.841180 -63.196436 -42.732187 -48.227991   \n",
       "8116 -20.810362 -22.059707 -17.869315 -21.630702 -26.578442 -20.143766   \n",
       "8117 -26.124367 -55.816301 -37.078218 -25.284482   0.382884 -14.342723   \n",
       "8118 -42.209128 -27.724533 -28.955970  -9.479079 -11.715716 -70.514130   \n",
       "8119 -42.256740 -33.737129 -10.603109 -26.980900   0.444167 -17.125913   \n",
       "\n",
       "             P7         P8         P9        P10 beach  \n",
       "6200  14.884869  30.097223 -18.873412   8.675822  CVCC  \n",
       "6201  -9.443531  -9.245408  -9.439674  12.999496  CCFT  \n",
       "6202   9.790987   0.029764 -10.467990   0.481180  FTAD  \n",
       "6203   1.536982   0.937429  -3.339971   7.273731  ADLA  \n",
       "6204  -8.991966  -0.814171 -20.100475 -11.294160  LABI  \n",
       "...         ...        ...        ...        ...   ...  \n",
       "8115   1.909535 -24.424192   1.786017 -28.490322  COSN  \n",
       "8116 -23.981757 -18.589730 -32.415639 -20.811116  VAGR  \n",
       "8117 -16.855074   3.686623 -29.390967 -38.576515  GBHA  \n",
       "8118 -48.037738 -49.362695 -53.586364 -36.346653  BARR  \n",
       "8119 -48.509492 -63.286528 -62.312174 -35.148936  MIRA  \n",
       "\n",
       "[1920 rows x 11 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Assuming 'y_pred' is a NumPy array or a pandas Series\n",
    "y_pred_df = pd.DataFrame(y_pred, index=x_test.index)\n",
    "y_pred_df.columns = ['P1', 'P2', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'P9', 'P10']\n",
    "\n",
    "# Assuming 'combined_data_1' is a DataFrame with a 'beach' column\n",
    "beach_names = combined_data_1['beach']\n",
    "\n",
    "# Create a DataFrame with the actual and predicted values\n",
    "# Make sure that the length of the index matches the length of 'y_pred'\n",
    "y_pred_df['beach'] = beach_names.iloc[y_pred_df.index].tolist()\n",
    "\n",
    "# Assuming 'y_test' is a NumPy array or a pandas Series\n",
    "y_test_df = pd.DataFrame(y_test, index=x_test.index)\n",
    "y_test_df['beach'] = beach_names.iloc[y_test_df.index].tolist()\n",
    "\n",
    "y_test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2645755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot figure\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.set_xlabel('Eastings')\n",
    "ax.set_ylabel('Northings')\n",
    "ax.grid(linestyle=':', color='0.5')\n",
    "\n",
    "# Plot transects\n",
    "for key, coordinates in transects.items():\n",
    "    ax.plot(coordinates['start'][0], coordinates['start'][1], 'bo', ms=5)\n",
    "    #ax.plot(coordinates['end'][0], coordinates['end'][1], 'bo', ms=5)\n",
    "    ax.plot([coordinates['start'][0], coordinates['end'][0]],\n",
    "            [coordinates['start'][1], coordinates['end'][1]], 'k-', lw=1)\n",
    "\n",
    "# Add OSM (OpenStreetMap) as background\n",
    "ctx.add_basemap(ax, crs=transects_gdf.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik, zoom=12)\n",
    "\n",
    "# Plot connected shoreline points for each year\n",
    "for year in shoreline_data.index.year.unique():\n",
    "    combined_points = []\n",
    "    for transect_name, coordinates in transects.items():\n",
    "        shoreline_value = shoreline_data.loc[shoreline_data.index.year == year, transect_name].values[0]\n",
    "        direction_vector = (coordinates['end'] - coordinates['start']) / np.linalg.norm(coordinates['end'] - coordinates['start'])\n",
    "        intersection_point = coordinates['start'] + shoreline_value * direction_vector\n",
    "        combined_points.append(intersection_point)\n",
    "    combined_points = np.array(combined_points)\n",
    "    ax.plot(combined_points[:, 0], combined_points[:, 1], label=str(year))\n",
    "\n",
    "# Customize your plot (labels, legend, etc.)\n",
    "ax.set_title('Connected Shoreline Points with Transects Over Time')\n",
    "#ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4f894d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:767: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.440673287413006"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and print the metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7717d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the RMSE values\n",
    "rmse_values = []\n",
    "\n",
    "# Iterate through each unique site_name\n",
    "for site_name in site_names:\n",
    "    # Filter the DataFrame for the specific site_name\n",
    "    y_test_site = y_test_df[y_test_df['beach'] == site_name].drop('beach', axis=1)  # Exclude 'beach' column\n",
    "    y_pred_site = y_pred_df[y_pred_df['beach'] == site_name].drop('beach', axis=1)  # Exclude 'beach' column\n",
    "    \n",
    "    # Check if DataFrames are not empty before calculating RMSE\n",
    "    if not y_test_site.empty and not y_pred_site.empty:\n",
    "        # Calculate RMSE for the current site_name\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_site.values.flatten(), y_pred_site.values.flatten()))\n",
    "        \n",
    "        # Append the RMSE value to the list\n",
    "        rmse_values.append({'site_name': site_name, 'rmse': rmse})\n",
    "\n",
    "# Create a DataFrame from the list of RMSE values\n",
    "rmse_df = pd.DataFrame(rmse_values)\n",
    "\n",
    "# Display the RMSE DataFrame\n",
    "print(rmse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417581b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation com time series split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c444f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline dummy que prevê o ano a seguir com base no anterior (igual)\n",
    "\n",
    "# Splitting the combined data into training and testing sets - Naive model to compare\n",
    "\n",
    "# Train for 2021\n",
    "training_data_naive = combined_data_1[combined_data_1['years'] == 2021]\n",
    "\n",
    "# Test for 2022\n",
    "testing_data_naive = combined_data_1[combined_data_1['years'] == 2022]\n",
    "\n",
    "# Define the columns for the target variables\n",
    "target_columns = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "# Initialize an empty list to store dictionaries for naive predictions\n",
    "naive_predictions = []\n",
    "\n",
    "# Iterate over each beach and month, and use the 2021 value as the prediction for 2022\n",
    "for beach in training_data_naive['beach'].unique():\n",
    "    for month in range(1, 13):\n",
    "        # Extract the last entry for the month in 2021 for the current beach\n",
    "        last_entry = training_data_naive[(training_data_naive['beach'] == beach) & \n",
    "                                         (training_data_naive['months'] == month)][target_columns].iloc[-1]\n",
    "\n",
    "        # Add the 'beach' and 'months' columns to last_entry for alignment with the test data\n",
    "        last_entry['beach'] = beach\n",
    "        last_entry['months'] = month\n",
    "\n",
    "        # Append this entry to the predictions list\n",
    "        naive_predictions.append(last_entry)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "y_pred_naive = pd.DataFrame(naive_predictions)\n",
    "\n",
    "# Align y_pred_naive with testing_data_naive\n",
    "y_pred_naive = y_pred_naive.set_index(['beach', 'months'])\n",
    "testing_data_naive = testing_data_naive.set_index(['beach', 'months'])\n",
    "y_test_naive = testing_data_naive[target_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02555937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE for the naive model\n",
    "rmse_naive = mean_squared_error(y_test_naive, y_pred_naive, squared=False)\n",
    "rmse_naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87230d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the RMSE values\n",
    "rmse_values = []\n",
    "\n",
    "# Iterate through each unique site_name\n",
    "for site_name in y_test_naive.index.get_level_values('beach').unique():\n",
    "    # Filter the DataFrame for the specific site_name\n",
    "    y_test_site = y_test_naive.loc[site_name]\n",
    "    y_pred_site = y_pred_naive.loc[site_name]\n",
    "    \n",
    "    # Calculate the RMSE for the current site_name\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_site, y_pred_site))\n",
    "    \n",
    "    # Append the RMSE value to the list\n",
    "    rmse_values.append({'site_name': site_name, 'rmse': rmse})\n",
    "\n",
    "# Create a DataFrame from the list of RMSE values\n",
    "rmse_df = pd.DataFrame(rmse_values)\n",
    "\n",
    "# Display the RMSE DataFrame\n",
    "print(rmse_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd51a8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering - create a column that represents time passing\n",
    "combined_data['elapsed_time'] = (combined_data['years'] - combined_data['years'].min()) * 12 + combined_data['months']\n",
    "\n",
    "\n",
    "# Filter rows for training data (years <= 2014)\n",
    "training_data_2 = combined_data[combined_data['years'] <= 2014]\n",
    "\n",
    "# Filter rows for testing data (years > 2014)\n",
    "testing_data_2 = combined_data[combined_data['years'] > 2014]\n",
    "\n",
    "# Define the columns for the target variables\n",
    "target_columns = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "\n",
    "# For training set\n",
    "x_train_2 = training_data_2.drop(target_columns + ['years', 'months'], axis=1)\n",
    "y_train_2 = training_data_2[target_columns]\n",
    "\n",
    "# For testing set\n",
    "x_test_2 = testing_data_2.drop(target_columns + ['years', 'months'], axis=1)\n",
    "y_test_2 = testing_data_2[target_columns]\n",
    "\n",
    "# Creating and training the model\n",
    "model_2 = DecisionTreeRegressor(random_state=42)\n",
    "model_2.fit(x_train_2, y_train_2)\n",
    "\n",
    "# Making predictions and evaluating the model\n",
    "y_pred_2 = model_2.predict(x_test_2)\n",
    "mse_2 = mean_squared_error(y_test_2, y_pred_2)\n",
    "rmse_2 = mean_squared_error(y_test_2, y_pred_2, squared=False)\n",
    "\n",
    "\n",
    "print(\"Root Mean Squared Error:\", rmse_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af511441",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2_df = pd.DataFrame(y_pred_2, index=x_test_2.index)\n",
    "y_pred_2_df.columns = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "beach_names = combined_data_1['beach']\n",
    "\n",
    "# Create a DataFrame with the actual and predicted values\n",
    "y_pred_2_df['beach'] = beach_names.iloc[df.index].tolist()\n",
    "\n",
    "y_test_2_df = pd.DataFrame(y_test_2)\n",
    "y_test_2_df['beach'] = beach_names.iloc[df.index].tolist()\n",
    "\n",
    "# Create a list to store the RMSE values\n",
    "rmse_values = []\n",
    "\n",
    "# Iterate through each unique site_name\n",
    "for site_name in site_names:\n",
    "    # Filter the DataFrame for the specific site_name\n",
    "    y_test_2_site = y_test_2_df[y_test_2_df['beach'] == site_name].drop('beach', axis=1)  # Exclude 'beach' column\n",
    "    y_pred_2_site = y_pred_2_df[y_pred_2_df['beach'] == site_name].drop('beach', axis=1)  # Exclude 'beach' column\n",
    "    \n",
    "    # Check if DataFrames are not empty before calculating RMSE\n",
    "    if not y_test_2_site.empty and not y_pred_2_site.empty:\n",
    "        # Calculate RMSE for the current site_name\n",
    "        rmse_2 = np.sqrt(mean_squared_error(y_test_2_site.values.flatten(), y_pred_2_site.values.flatten()))\n",
    "        \n",
    "        # Append the RMSE value to the list\n",
    "        rmse_values.append({'site_name': site_name, 'rmse': rmse_2})\n",
    "\n",
    "# Create a DataFrame from the list of RMSE values\n",
    "rmse_df_2 = pd.DataFrame(rmse_values)\n",
    "\n",
    "# Display the RMSE DataFrame\n",
    "print(rmse_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a6f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST MODEL\n",
    "\n",
    "# Creating and training the Random Forest model\n",
    "model_rf = RandomForestRegressor(random_state=42)\n",
    "model_rf.fit(x_train_2, y_train_2)\n",
    "\n",
    "# Making predictions and evaluating the model\n",
    "y_pred_rf = model_rf.predict(x_test_2)\n",
    "mse_rf = mean_squared_error(y_test_2, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_test_2, y_pred_rf, squared=False)\n",
    "\n",
    "print(\"Root Mean Squared Error with Random Forest:\", rmse_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ba45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf_df = pd.DataFrame(y_pred_rf, index=x_test_2.index)\n",
    "y_pred_rf_df .columns = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "beach_names = combined_data_1['beach']\n",
    "\n",
    "# Create a DataFrame with the actual and predicted values\n",
    "y_pred_rf_df['beach'] = beach_names.iloc[df.index].tolist()\n",
    "\n",
    "y_test_2_df = pd.DataFrame(y_test_2)\n",
    "y_test_2_df['beach'] = beach_names.iloc[df.index].tolist()\n",
    "\n",
    "# Create a list to store the RMSE values\n",
    "rmse_values = []\n",
    "\n",
    "# Iterate through each unique site_name\n",
    "for site_name in site_names:\n",
    "    # Filter the DataFrame for the specific site_name\n",
    "    y_test_2_site = y_test_2_df[y_test_2_df['beach'] == site_name].drop('beach', axis=1)  # Exclude 'beach' column\n",
    "    y_pred_rf_df_site = y_pred_rf_df[y_pred_rf_df['beach'] == site_name].drop('beach', axis=1)  # Exclude 'beach' column\n",
    "    \n",
    "    # Check if DataFrames are not empty before calculating RMSE\n",
    "    if not y_test_2_site.empty and not y_pred_rf_df_site.empty:\n",
    "        # Calculate RMSE for the current site_name\n",
    "        rmse_2 = np.sqrt(mean_squared_error(y_test_2_site.values.flatten(), y_pred_rf_df_site.values.flatten()))\n",
    "        \n",
    "        # Append the RMSE value to the list\n",
    "        rmse_values.append({'site_name': site_name, 'rmse': rmse_2})\n",
    "\n",
    "# Create a DataFrame from the list of RMSE values\n",
    "rmse_df_2 = pd.DataFrame(rmse_values)\n",
    "\n",
    "# Display the RMSE DataFrame\n",
    "print(rmse_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time series cross-validation ensures that each test set is 'ahead in time' relative to its corresponding training set, \n",
    "#thereby better mimicking the scenario where you predict future values based on past observations. \n",
    "#This method provides a more robust and realistic evaluation of your model's performance over time.\n",
    "\n",
    "# Define the columns for the target variables and features\n",
    "target_columns = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "feature_columns = [col for col in combined_data.columns if col not in target_columns + ['years', 'months']]\n",
    "\n",
    "# Preparing the dataset\n",
    "X = combined_data[feature_columns]\n",
    "y = combined_data[target_columns]\n",
    "\n",
    "# Create time series cross-validator object\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Initialize lists to store results\n",
    "rmse_scores = []\n",
    "\n",
    "# Loop through each split\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Create and fit the Random Forest model\n",
    "    model_cv = RandomForestRegressor(random_state=42)\n",
    "    model_cv.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    # Make predictions and evaluate\n",
    "    y_pred_cv = model_cv.predict(X_test_cv)\n",
    "    rmse_cv = mean_squared_error(y_test_cv, y_pred_cv, squared=False)\n",
    "    rmse_scores.append(rmse_cv)\n",
    "\n",
    "# Calculate average RMSE\n",
    "average_rmse = np.mean(rmse_scores)\n",
    "print(\"Average RMSE with Time Series Cross-Validation:\", average_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eaf2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
