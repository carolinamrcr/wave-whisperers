{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5950155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3d9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and site names setup\n",
    "waves_folder_path = \"./dataset_Ondas\"\n",
    "shorelines_folder_path = \"./dataset_linhascosta\"\n",
    "transects_folder_path = \"./dataset_transects\"\n",
    "site_names = [\"CCFT\", \"NNOR\", \"MEIA\", \"TROI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0190148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store DataFrames\n",
    "data, annual_data = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6aad260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing wave and shoreline data for each site\n",
    "for name in site_names:\n",
    "    # File paths\n",
    "    waves_file_path = os.path.join(waves_folder_path, f\"{name}_wave_timeseries.csv\")\n",
    "    shorelines_file_path = os.path.join(shorelines_folder_path, f\"{name}_shoreline_timeseries.csv\")\n",
    "    transects_file_path = os.path.join(transects_folder_path, f\"{name}_transects.geojson\")\n",
    "\n",
    "    # Read the waves CSV files into DataFrame\n",
    "    waves_df = pd.read_csv(waves_file_path, sep=',', header=0) # Set header=0 to use the first row as column headers\n",
    "    waves_df['time'] = pd.to_datetime(waves_df['time'])\n",
    "    waves_df.set_index('time', inplace=True)\n",
    "    waves_df['years'] = waves_df.index.year\n",
    "    waves_df['months'] = waves_df.index.month\n",
    "    waves_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(year, month) for year, month in zip(waves_df.index.year, waves_df.index.month)],\n",
    "    names=['years', 'months'])\n",
    "    waves_df = waves_df[waves_df['years'] != 1983] # Remove 1983 because satellite data for shorelines is not available for that year\n",
    "    \n",
    "    # List of directions (16 directions compass rose)\n",
    "    directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    def degrees_to_direction(wave_direction_degrees):\n",
    "        if wave_direction_degrees >= 0 and   wave_direction_degrees <= 11.25:\n",
    "            return 'N'\n",
    "        elif wave_direction_degrees <= 33.75:\n",
    "            return 'NNE'\n",
    "        elif wave_direction_degrees <= 56.25:\n",
    "            return 'NE'\n",
    "        elif wave_direction_degrees <= 78.75:\n",
    "            return 'ENE'\n",
    "        elif wave_direction_degrees <= 101.25:\n",
    "            return 'E'\n",
    "        elif wave_direction_degrees <= 123.75:\n",
    "            return 'ESE'\n",
    "        elif wave_direction_degrees <= 146.25:\n",
    "            return 'SE'\n",
    "        elif wave_direction_degrees <= 168.75:\n",
    "            return 'SSE'\n",
    "        elif wave_direction_degrees <= 191.25:\n",
    "            return 'S'\n",
    "        elif wave_direction_degrees <= 213.75:\n",
    "            return 'SSW'\n",
    "        elif wave_direction_degrees <= 236.25:\n",
    "            return 'SW'\n",
    "        elif wave_direction_degrees <= 258.75:\n",
    "            return 'WSW'\n",
    "        elif wave_direction_degrees <= 281.25:\n",
    "            return 'W'\n",
    "        elif wave_direction_degrees <= 303.75:\n",
    "            return 'WNW'\n",
    "        elif wave_direction_degrees <= 326.25:\n",
    "            return 'NW'\n",
    "        elif wave_direction_degrees <= 348.75:\n",
    "            return 'NNW'\n",
    "        elif wave_direction_degrees <= 360:\n",
    "            return 'N'\n",
    "        else:\n",
    "            return 'false'\n",
    "  \n",
    "    # One-hot encode the 'mwd' column\n",
    "    waves_df['mwd'] = waves_df['mwd'].apply(degrees_to_direction)\n",
    "\n",
    "    # Create a DataFrame of dummy variables for 'mwd'\n",
    "    one_hot_encode = pd.get_dummies(waves_df['mwd'], prefix='from')\n",
    "\n",
    "    # Concatenate the one-hot encoded columns to the original DataFrame\n",
    "    waves_df = pd.concat([waves_df, one_hot_encode], axis=1)\n",
    "    waves_df = waves_df.drop('mwd', axis=1)\n",
    "\n",
    "    # Iterate through directions and create new columns for each direction's pp1d and swh\n",
    "    for direction in directions:\n",
    "        # Create new columns for pp1d and swh\n",
    "        pp1d_column_name = f'pp1d_from_{direction}'\n",
    "        swh_column_name = f'swh_from_{direction}'\n",
    "    \n",
    "        # Use boolean indexing to set values based on the condition\n",
    "        waves_df[pp1d_column_name] = waves_df['pp1d'] * waves_df[f'from_{direction}']\n",
    "        waves_df[swh_column_name] = waves_df['swh'] * waves_df[f'from_{direction}']\n",
    "    \n",
    "    # Drop the original 'mwd' column and the 'pp1d' and 'swh' columns\n",
    "    waves_df.drop(columns=[f'from_{direction}' for direction in directions], inplace=True)\n",
    "    waves_df.drop(columns=['pp1d','swh'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Read the shorelines CSV files into DataFrame\n",
    "    shorelines_df = pd.read_csv(shorelines_file_path)\n",
    "    shorelines_df = shorelines_df.iloc[:, 1:]\n",
    "    shorelines_df['dates'] = pd.to_datetime(shorelines_df['dates'])\n",
    "    shorelines_df.set_index('dates', inplace=True)\n",
    "    shorelines_df['years'] = shorelines_df.index.year\n",
    "    shorelines_df['months'] = shorelines_df.index.month\n",
    "    shorelines_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(year, month) for year, month in zip(shorelines_df.index.year, shorelines_df.index.month)],\n",
    "    names=['years', 'months'])\n",
    "    \n",
    "    # Add a new column to waves and shorelines dataframes to indicate the site name\n",
    "    waves_df['site'] = name\n",
    "    shorelines_df['site'] = name\n",
    "\n",
    "    # Read the transects GeoJSON file into a GeoDataFrame\n",
    "    transects_gdf = gpd.read_file(transects_file_path, driver='GeoJSON')\n",
    "\n",
    "    # Add DataFrames to the dictionary with site name as key\n",
    "    data[name] = {\n",
    "        'waves': waves_df,\n",
    "        'shorelines': shorelines_df,\n",
    "        'transects': transects_gdf\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150eca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over keys in the data dictionary\n",
    "for name in data.keys():\n",
    "    waves_df = data[name]['waves']\n",
    "    \n",
    "    # Group by 'years' and calculate quantiles for each column\n",
    "    wave_df_annual = waves_df.groupby(level=['years', 'months']).agg(\n",
    "           {\n",
    "        'pp1d_from_N': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_N': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NNE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NNE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_ENE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_ENE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_E': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_E': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_ESE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_ESE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ], \n",
    "        'swh_from_SE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SSE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SSE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_S': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_S': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_SW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_WSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_WSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_W': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_W': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_WNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_WNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ]})\n",
    "    \n",
    "\n",
    "    # Replace NaN values with zero\n",
    "    wave_df_annual = wave_df_annual.fillna(0)\n",
    "\n",
    "    shoreline_df = data[name]['shorelines']\n",
    "\n",
    "    # Group by 'years' and calculate median for each column\n",
    "    shoreline_df_annual = shoreline_df.groupby(level=['years', 'months']).median(numeric_only=True)\n",
    "    \n",
    "    # Drop year and month columns\n",
    "    shoreline_df_annual = shoreline_df_annual.drop(['years', 'months'], axis=1)\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "\n",
    "    for i in range(1, len(shoreline_df_annual.columns) - 1):\n",
    "        col = shoreline_df_annual.columns[i]\n",
    "        prev_col = shoreline_df_annual.columns[i - 1] if i - 1 >= 0 else None\n",
    "        next_col = shoreline_df_annual.columns[i + 1] if i + 1 < len(shoreline_df_annual.columns) else None\n",
    "\n",
    "        # Check if there are any NaN values in the current column\n",
    "        if shoreline_df_annual[col].isnull().any():\n",
    "            # Fill NaN values with the mean of the available previous and next columns\n",
    "            if prev_col is not None and next_col is not None:\n",
    "                shoreline_df_annual[col] = (shoreline_df_annual[prev_col] + shoreline_df_annual[next_col]) / 2\n",
    "            elif prev_col is not None:\n",
    "                shoreline_df_annual[col] = shoreline_df_annual[prev_col]\n",
    "            elif next_col is not None:\n",
    "                shoreline_df_annual[col] = shoreline_df_annual[next_col]\n",
    "            else:\n",
    "                # If there are no immediate previous and next columns, extend the search to 3 columns\n",
    "                prev_cols = [shoreline_df_annual.columns[j] for j in range(i - 2, i) if j >= 0]\n",
    "                next_cols = [shoreline_df_annual.columns[j] for j in range(i + 1, i + 4) if j < len(shoreline_df_annual.columns)]\n",
    "\n",
    "                available_cols = prev_cols + next_cols\n",
    "\n",
    "                # Filter out None values (columns that are out of range)\n",
    "                available_cols = [col for col in available_cols if col is not None]\n",
    "\n",
    "                # Take the mean of available columns\n",
    "                if len(available_cols) > 0:\n",
    "                    shoreline_df_annual[col] = shoreline_df_annual[available_cols].mean(axis=1)\n",
    "                    \n",
    "        \n",
    "        for column in shoreline_df_annual.columns:\n",
    "        # Check if there are any NaN values in the column\n",
    "        if shoreline_df_annual[column].isnull().any():\n",
    "            \n",
    "            # Final check and fill/drop remaining NaNs\n",
    "            shoreline_df_annual.fillna(method='ffill', inplace=True)\n",
    "            shoreline_df_annual.fillna(method='bfill', inplace=True)\n",
    "            \n",
    "            # Calculate the median value of the column (excluding NaN values)\n",
    "            median_value = shoreline_df_annual[column].median()\n",
    "        \n",
    "            # Replace NaN values with the calculated median value\n",
    "            shoreline_df_annual[column].fillna(median_value, inplace=True)\n",
    "\n",
    "        # Ensure no NaNs are left before model training\n",
    "        if shoreline_df_annual.isna().any().any():\n",
    "            print(f\"NaNs remain in shorelines data for {name}\")\n",
    "            continue  # Skip this iteration if NaNs are still present\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add the DataFrame to the dictionary with site name as key\n",
    "    annual_data[name] = {\n",
    "        'waves': wave_df_annual,\n",
    "        'shorelines': shoreline_df_annual\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda8c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79c0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ddf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69218edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8563108e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4021fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">pp1d_from_N</th>\n",
       "      <th colspan=\"3\" halign=\"left\">swh_from_N</th>\n",
       "      <th colspan=\"3\" halign=\"left\">pp1d_from_NNE</th>\n",
       "      <th>swh_from_NNE</th>\n",
       "      <th>...</th>\n",
       "      <th>pp1d_from_NW</th>\n",
       "      <th colspan=\"3\" halign=\"left\">swh_from_NW</th>\n",
       "      <th colspan=\"3\" halign=\"left\">pp1d_from_NNW</th>\n",
       "      <th colspan=\"3\" halign=\"left\">swh_from_NNW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>...</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <th>months</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1984</th>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.568388</td>\n",
       "      <td>1.159329</td>\n",
       "      <td>2.113522</td>\n",
       "      <td>3.003425</td>\n",
       "      <td>10.858882</td>\n",
       "      <td>11.489670</td>\n",
       "      <td>12.711223</td>\n",
       "      <td>0.784305</td>\n",
       "      <td>1.325311</td>\n",
       "      <td>2.069451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.00150</td>\n",
       "      <td>11.651595</td>\n",
       "      <td>12.257721</td>\n",
       "      <td>0.581329</td>\n",
       "      <td>0.698613</td>\n",
       "      <td>0.849621</td>\n",
       "      <td>11.091750</td>\n",
       "      <td>12.218543</td>\n",
       "      <td>12.235737</td>\n",
       "      <td>0.602527</td>\n",
       "      <td>...</td>\n",
       "      <td>15.556421</td>\n",
       "      <td>0.944696</td>\n",
       "      <td>1.942769</td>\n",
       "      <td>2.496285</td>\n",
       "      <td>8.990899</td>\n",
       "      <td>10.270372</td>\n",
       "      <td>12.130041</td>\n",
       "      <td>0.648540</td>\n",
       "      <td>1.061916</td>\n",
       "      <td>1.478057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.03587</td>\n",
       "      <td>10.035870</td>\n",
       "      <td>10.035870</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>10.085448</td>\n",
       "      <td>10.130714</td>\n",
       "      <td>10.176769</td>\n",
       "      <td>0.842339</td>\n",
       "      <td>...</td>\n",
       "      <td>16.012798</td>\n",
       "      <td>0.664541</td>\n",
       "      <td>1.757484</td>\n",
       "      <td>2.532742</td>\n",
       "      <td>10.067748</td>\n",
       "      <td>10.614656</td>\n",
       "      <td>11.018130</td>\n",
       "      <td>0.748528</td>\n",
       "      <td>0.768114</td>\n",
       "      <td>0.841217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.969103</td>\n",
       "      <td>0.695217</td>\n",
       "      <td>1.110330</td>\n",
       "      <td>1.613585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.222614</td>\n",
       "      <td>0.719590</td>\n",
       "      <td>1.001971</td>\n",
       "      <td>1.661999</td>\n",
       "      <td>7.460350</td>\n",
       "      <td>7.915628</td>\n",
       "      <td>11.252801</td>\n",
       "      <td>0.975197</td>\n",
       "      <td>1.129680</td>\n",
       "      <td>1.177368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022</th>\n",
       "      <th>8</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.344798</td>\n",
       "      <td>0.656596</td>\n",
       "      <td>0.987281</td>\n",
       "      <td>1.394260</td>\n",
       "      <td>8.006328</td>\n",
       "      <td>8.218338</td>\n",
       "      <td>8.557211</td>\n",
       "      <td>0.701709</td>\n",
       "      <td>0.809183</td>\n",
       "      <td>0.956542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.501251</td>\n",
       "      <td>1.084615</td>\n",
       "      <td>1.306514</td>\n",
       "      <td>1.547684</td>\n",
       "      <td>8.186714</td>\n",
       "      <td>8.565018</td>\n",
       "      <td>8.682043</td>\n",
       "      <td>1.401431</td>\n",
       "      <td>1.507832</td>\n",
       "      <td>1.520468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.812888</td>\n",
       "      <td>0.663309</td>\n",
       "      <td>1.016424</td>\n",
       "      <td>1.373330</td>\n",
       "      <td>6.105875</td>\n",
       "      <td>6.203791</td>\n",
       "      <td>8.034147</td>\n",
       "      <td>0.661129</td>\n",
       "      <td>0.741183</td>\n",
       "      <td>0.833177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.569991</td>\n",
       "      <td>1.552865</td>\n",
       "      <td>1.781020</td>\n",
       "      <td>2.520800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pp1d_from_N                                swh_from_N  \\\n",
       "             10th_quantile 50th_quantile 90th_quantile 10th_quantile   \n",
       "years months                                                           \n",
       "1984  1            0.00000      0.000000      0.000000      0.000000   \n",
       "      2           11.00150     11.651595     12.257721      0.581329   \n",
       "      3           10.03587     10.035870     10.035870      0.782094   \n",
       "      4            0.00000      0.000000      0.000000      0.000000   \n",
       "      5            0.00000      0.000000      0.000000      0.000000   \n",
       "...                    ...           ...           ...           ...   \n",
       "2022  8            0.00000      0.000000      0.000000      0.000000   \n",
       "      9            0.00000      0.000000      0.000000      0.000000   \n",
       "      10           0.00000      0.000000      0.000000      0.000000   \n",
       "      11           0.00000      0.000000      0.000000      0.000000   \n",
       "      12           0.00000      0.000000      0.000000      0.000000   \n",
       "\n",
       "                                         pp1d_from_NNE                \\\n",
       "             50th_quantile 90th_quantile 10th_quantile 50th_quantile   \n",
       "years months                                                           \n",
       "1984  1           0.000000      0.000000      0.000000      0.000000   \n",
       "      2           0.698613      0.849621     11.091750     12.218543   \n",
       "      3           0.782094      0.782094     10.085448     10.130714   \n",
       "      4           0.000000      0.000000      0.000000      0.000000   \n",
       "      5           0.000000      0.000000      0.000000      0.000000   \n",
       "...                    ...           ...           ...           ...   \n",
       "2022  8           0.000000      0.000000      0.000000      0.000000   \n",
       "      9           0.000000      0.000000      0.000000      0.000000   \n",
       "      10          0.000000      0.000000      0.000000      0.000000   \n",
       "      11          0.000000      0.000000      0.000000      0.000000   \n",
       "      12          0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "                            swh_from_NNE  ...  pp1d_from_NW   swh_from_NW  \\\n",
       "             90th_quantile 10th_quantile  ... 90th_quantile 10th_quantile   \n",
       "years months                              ...                               \n",
       "1984  1           0.000000      0.000000  ...     16.568388      1.159329   \n",
       "      2          12.235737      0.602527  ...     15.556421      0.944696   \n",
       "      3          10.176769      0.842339  ...     16.012798      0.664541   \n",
       "      4           0.000000      0.000000  ...     11.969103      0.695217   \n",
       "      5           0.000000      0.000000  ...     11.222614      0.719590   \n",
       "...                    ...           ...  ...           ...           ...   \n",
       "2022  8           0.000000      0.000000  ...     11.344798      0.656596   \n",
       "      9           0.000000      0.000000  ...     16.501251      1.084615   \n",
       "      10          0.000000      0.000000  ...     12.812888      0.663309   \n",
       "      11          0.000000      0.000000  ...     14.569991      1.552865   \n",
       "      12          0.000000      0.000000  ...      0.000000      0.000000   \n",
       "\n",
       "                                         pp1d_from_NNW                \\\n",
       "             50th_quantile 90th_quantile 10th_quantile 50th_quantile   \n",
       "years months                                                           \n",
       "1984  1           2.113522      3.003425     10.858882     11.489670   \n",
       "      2           1.942769      2.496285      8.990899     10.270372   \n",
       "      3           1.757484      2.532742     10.067748     10.614656   \n",
       "      4           1.110330      1.613585      0.000000      0.000000   \n",
       "      5           1.001971      1.661999      7.460350      7.915628   \n",
       "...                    ...           ...           ...           ...   \n",
       "2022  8           0.987281      1.394260      8.006328      8.218338   \n",
       "      9           1.306514      1.547684      8.186714      8.565018   \n",
       "      10          1.016424      1.373330      6.105875      6.203791   \n",
       "      11          1.781020      2.520800      0.000000      0.000000   \n",
       "      12          0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "                            swh_from_NNW                              \n",
       "             90th_quantile 10th_quantile 50th_quantile 90th_quantile  \n",
       "years months                                                          \n",
       "1984  1          12.711223      0.784305      1.325311      2.069451  \n",
       "      2          12.130041      0.648540      1.061916      1.478057  \n",
       "      3          11.018130      0.748528      0.768114      0.841217  \n",
       "      4           0.000000      0.000000      0.000000      0.000000  \n",
       "      5          11.252801      0.975197      1.129680      1.177368  \n",
       "...                    ...           ...           ...           ...  \n",
       "2022  8           8.557211      0.701709      0.809183      0.956542  \n",
       "      9           8.682043      1.401431      1.507832      1.520468  \n",
       "      10          8.034147      0.661129      0.741183      0.833177  \n",
       "      11          0.000000      0.000000      0.000000      0.000000  \n",
       "      12          0.000000      0.000000      0.000000      0.000000  \n",
       "\n",
       "[468 rows x 96 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annual_data['TROI']['waves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f2d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine standardized data into a single empty DataFrame with all months and years from the previous tables\n",
    "combined_shorelines = pd.DataFrame(index=pd.MultiIndex.from_product(\n",
    "    [shoreline_df.index.get_level_values(0).unique(), shoreline_df.index.get_level_values(1).unique()],\n",
    "    names=['years', 'months']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3ae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from all sites - SHORELINES\n",
    "for name in site_names:\n",
    "    df_to_merge = annual_data[name]['shorelines']\n",
    "\n",
    "    # Reset index if 'years' and 'months' are part of the index\n",
    "    if 'years' in df_to_merge.index.names and 'months' in df_to_merge.index.names:\n",
    "        df_to_merge = df_to_merge.reset_index()\n",
    "\n",
    "    combined_shorelines = combined_shorelines.merge(df_to_merge, \n",
    "                                                   on=['years', 'months'], \n",
    "                                                   how='left')\n",
    "\n",
    "# Handling NaNs - Group by 'years' and fill NaNs with the mean of the respective year\n",
    "for column in combined_shorelines.columns:\n",
    "    if column not in ['site', 'years', 'months']:\n",
    "        combined_shorelines[column] = combined_shorelines.groupby('years')[column].transform(lambda x: x.fillna(x.mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de126946",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_df_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure that the waves dataframe has the same dimensions as the shorelines\n",
    "\n",
    "combined_waves = pd.DataFrame(index=pd.MultiIndex.from_product(\n",
    "    [shoreline_df.index.get_level_values(0).unique(), shoreline_df.index.get_level_values(1).unique()],\n",
    "    names=['years', 'months']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from all sites - SHORELINES\n",
    "\n",
    "df_to_merge_waves = wave_df_annual\n",
    "\n",
    "# Reset index if 'years' and 'months' are part of the index\n",
    "if 'years' in df_to_merge_waves.index and 'months' in df_to_merge.index_waves:\n",
    "    df_to_merge_waves = df_to_merge_waves.reset_index()\n",
    "\n",
    "combined_waves = combined_waves.merge(df_to_merge_waves, \n",
    "                                      on=['years', 'months'], \n",
    "                                      how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9371e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#combined_waves.shape\n",
    "combined_shorelines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = combined_waves  # Assuming 'combined_waves' contains your features\n",
    "y = combined_shorelines  # Assuming 'combined_shorelines' contains your target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ddb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b378d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassigning multi-index to combined_shorelines table\n",
    "#combined_shorelines = combined_shorelines.set_index(['years', 'months'])\n",
    "combined_shorelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "\n",
    "# Create training datasets\n",
    "x_train = combined_waves[combined_waves.index.get_level_values(0) <= 2014]\n",
    "y_train = combined_shorelines[combined_shorelines.index.get_level_values(0) <= 2014]\n",
    "\n",
    "# Create testing datasets\n",
    "x_test = combined_waves[combined_waves.index.get_level_values(0) > 2014]\n",
    "y_test = combined_shorelines[combined_shorelines.index.get_level_values(0) > 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d35b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edd0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DecisionTreeRegressor model\n",
    "model = DecisionTreeRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eceeae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict shoreline positions\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = pd.DataFrame(y_pred, columns=y_test.columns, index=y_test.index)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94fd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd02c13a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe1bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c4fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2affb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0faffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50615490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65c186a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee59191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b456d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
