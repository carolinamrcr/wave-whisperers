{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5950155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc3d9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and site names setup\n",
    "waves_folder_path = \"./dataset_Ondas\"\n",
    "shorelines_folder_path = \"./dataset_linhascosta\"\n",
    "transects_folder_path = \"./dataset_transects\"\n",
    "site_names = [\"CCFT\", \"NNOR\", \"MEIA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0190148c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store DataFrames\n",
    "data, annual_data = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6aad260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing wave and shoreline data for each site\n",
    "for name in site_names:\n",
    "    # File paths\n",
    "    waves_file_path = os.path.join(waves_folder_path, f\"{name}_wave_timeseries.csv\")\n",
    "    shorelines_file_path = os.path.join(shorelines_folder_path, f\"{name}_shoreline_timeseries.csv\")\n",
    "    transects_file_path = os.path.join(transects_folder_path, f\"{name}_transects.geojson\")\n",
    "\n",
    "    # Read the waves CSV files into DataFrame\n",
    "    waves_df = pd.read_csv(waves_file_path, sep=',', header=0) # Set header=0 to use the first row as column headers\n",
    "    waves_df['time'] = pd.to_datetime(waves_df['time'])\n",
    "    waves_df.set_index('time', inplace=True)\n",
    "    waves_df['years'] = waves_df.index.year\n",
    "    waves_df['months'] = waves_df.index.month\n",
    "    waves_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(year, month) for year, month in zip(waves_df.index.year, waves_df.index.month)],\n",
    "    names=['years', 'months'])\n",
    "    waves_df = waves_df[waves_df['years'] != 1983] # Remove 1983 because satellite data for shorelines is not available for that year\n",
    "    \n",
    "    # List of directions (16 directions compass rose)\n",
    "    directions = ['N', 'NNE', 'NE', 'ENE', 'E', 'ESE', 'SE', 'SSE', 'S', 'SSW', 'SW', 'WSW', 'W', 'WNW', 'NW', 'NNW']\n",
    "    def degrees_to_direction(wave_direction_degrees):\n",
    "        if wave_direction_degrees >= 0 and   wave_direction_degrees <= 11.25:\n",
    "            return 'N'\n",
    "        elif wave_direction_degrees <= 33.75:\n",
    "            return 'NNE'\n",
    "        elif wave_direction_degrees <= 56.25:\n",
    "            return 'NE'\n",
    "        elif wave_direction_degrees <= 78.75:\n",
    "            return 'ENE'\n",
    "        elif wave_direction_degrees <= 101.25:\n",
    "            return 'E'\n",
    "        elif wave_direction_degrees <= 123.75:\n",
    "            return 'ESE'\n",
    "        elif wave_direction_degrees <= 146.25:\n",
    "            return 'SE'\n",
    "        elif wave_direction_degrees <= 168.75:\n",
    "            return 'SSE'\n",
    "        elif wave_direction_degrees <= 191.25:\n",
    "            return 'S'\n",
    "        elif wave_direction_degrees <= 213.75:\n",
    "            return 'SSW'\n",
    "        elif wave_direction_degrees <= 236.25:\n",
    "            return 'SW'\n",
    "        elif wave_direction_degrees <= 258.75:\n",
    "            return 'WSW'\n",
    "        elif wave_direction_degrees <= 281.25:\n",
    "            return 'W'\n",
    "        elif wave_direction_degrees <= 303.75:\n",
    "            return 'WNW'\n",
    "        elif wave_direction_degrees <= 326.25:\n",
    "            return 'NW'\n",
    "        elif wave_direction_degrees <= 348.75:\n",
    "            return 'NNW'\n",
    "        elif wave_direction_degrees <= 360:\n",
    "            return 'N'\n",
    "        else:\n",
    "            return 'false'\n",
    "  \n",
    "    # One-hot encode the 'mwd' column\n",
    "    waves_df['mwd'] = waves_df['mwd'].apply(degrees_to_direction)\n",
    "\n",
    "    # Create a DataFrame of dummy variables for 'mwd'\n",
    "    one_hot_encode = pd.get_dummies(waves_df['mwd'], prefix='from')\n",
    "\n",
    "    # Concatenate the one-hot encoded columns to the original DataFrame\n",
    "    waves_df = pd.concat([waves_df, one_hot_encode], axis=1)\n",
    "    waves_df = waves_df.drop('mwd', axis=1)\n",
    "\n",
    "    # Iterate through directions and create new columns for each direction's pp1d and swh\n",
    "    for direction in directions:\n",
    "        # Create new columns for pp1d and swh\n",
    "        pp1d_column_name = f'pp1d_from_{direction}'\n",
    "        swh_column_name = f'swh_from_{direction}'\n",
    "    \n",
    "        # Use boolean indexing to set values based on the condition\n",
    "        waves_df[pp1d_column_name] = waves_df['pp1d'] * waves_df[f'from_{direction}']\n",
    "        waves_df[swh_column_name] = waves_df['swh'] * waves_df[f'from_{direction}']\n",
    "    \n",
    "    # Drop the original 'mwd' column and the 'pp1d' and 'swh' columns\n",
    "    waves_df.drop(columns=[f'from_{direction}' for direction in directions], inplace=True)\n",
    "    waves_df.drop(columns=['pp1d','swh'], inplace=True)\n",
    "    \n",
    "    \n",
    "    # Read the shorelines CSV files into DataFrame\n",
    "    shorelines_df = pd.read_csv(shorelines_file_path)\n",
    "    shorelines_df = shorelines_df.iloc[:, 1:]\n",
    "    shorelines_df['dates'] = pd.to_datetime(shorelines_df['dates'])\n",
    "    shorelines_df.set_index('dates', inplace=True)\n",
    "    shorelines_df['years'] = shorelines_df.index.year\n",
    "    shorelines_df['months'] = shorelines_df.index.month\n",
    "    shorelines_df.index = pd.MultiIndex.from_tuples(\n",
    "    [(year, month) for year, month in zip(shorelines_df.index.year, shorelines_df.index.month)],\n",
    "    names=['years', 'months'])\n",
    "    \n",
    "    # Add a new column to waves and shorelines dataframes to indicate the site name\n",
    "    waves_df['site'] = name\n",
    "    shorelines_df['site'] = name\n",
    "\n",
    "    # Read the transects GeoJSON file into a GeoDataFrame\n",
    "    transects_gdf = gpd.read_file(transects_file_path, driver='GeoJSON')\n",
    "\n",
    "    # Add DataFrames to the dictionary with site name as key\n",
    "    data[name] = {\n",
    "        'waves': waves_df,\n",
    "        'shorelines': shorelines_df,\n",
    "        'transects': transects_gdf\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "150eca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over keys in the data dictionary\n",
    "for name in data.keys():\n",
    "    waves_df = data[name]['waves']\n",
    "    \n",
    "    # Group by 'years' and calculate quantiles for each column\n",
    "    wave_df_annual = waves_df.groupby(level=['years', 'months']).agg(\n",
    "           {\n",
    "        'pp1d_from_N': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_N': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NNE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NNE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_ENE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_ENE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_E': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_E': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_ESE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_ESE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ], \n",
    "        'swh_from_SE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SSE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SSE': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_S': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_S': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'pp1d_from_SSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_SW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_SW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_WSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None) \n",
    "        ],\n",
    "        'swh_from_WSW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_W': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_W': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_WNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_WNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None), \n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None), \n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'pp1d_from_NNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ],\n",
    "        'swh_from_NNW': [\n",
    "            ('10th_quantile', lambda x: x[x != 0].quantile(0.1) if any(x != 0) else None),\n",
    "            ('50th_quantile', lambda x: x[x != 0].quantile(0.5) if any(x != 0) else None),\n",
    "            ('90th_quantile', lambda x: x[x != 0].quantile(0.9) if any(x != 0) else None)\n",
    "        ]})\n",
    "    \n",
    "\n",
    "    # Replace NaN values with zero\n",
    "    wave_df_annual = wave_df_annual.fillna(0)\n",
    "\n",
    "    shoreline_df = data[name]['shorelines']\n",
    "\n",
    "    # Group by 'years' and calculate median for each column\n",
    "    shoreline_df_annual = shoreline_df.groupby(level=['years', 'months']).median(numeric_only=True)\n",
    "    \n",
    "    # Drop year and month columns\n",
    "    #shoreline_df_annual = shoreline_df_annual.drop(['years', 'months'], axis=1)\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "\n",
    "    for i in range(1, len(shoreline_df_annual.columns) - 1):\n",
    "        col = shoreline_df_annual.columns[i]\n",
    "        prev_col = shoreline_df_annual.columns[i - 1] if i - 1 >= 0 else None\n",
    "        next_col = shoreline_df_annual.columns[i + 1] if i + 1 < len(shoreline_df_annual.columns) else None\n",
    "\n",
    "        # Check if there are any NaN values in the current column\n",
    "        if shoreline_df_annual[col].isnull().any():\n",
    "            # Fill NaN values with the mean of the available previous and next columns\n",
    "            if prev_col is not None and next_col is not None:\n",
    "                shoreline_df_annual[col] = (shoreline_df_annual[prev_col] + shoreline_df_annual[next_col]) / 2\n",
    "            elif prev_col is not None:\n",
    "                shoreline_df_annual[col] = shoreline_df_annual[prev_col]\n",
    "            elif next_col is not None:\n",
    "                shoreline_df_annual[col] = shoreline_df_annual[next_col]\n",
    "            else:\n",
    "                # If there are no immediate previous and next columns, extend the search to 3 columns\n",
    "                prev_cols = [shoreline_df_annual.columns[j] for j in range(i - 2, i) if j >= 0]\n",
    "                next_cols = [shoreline_df_annual.columns[j] for j in range(i + 1, i + 4) if j < len(shoreline_df_annual.columns)]\n",
    "\n",
    "                available_cols = prev_cols + next_cols\n",
    "\n",
    "                # Filter out None values (columns that are out of range)\n",
    "                available_cols = [col for col in available_cols if col is not None]\n",
    "\n",
    "                # Take the mean of available columns\n",
    "                if len(available_cols) > 0:\n",
    "                    shoreline_df_annual[col] = shoreline_df_annual[available_cols].mean(axis=1)\n",
    "\n",
    "    for column in shoreline_df_annual.columns:\n",
    "        # Check if there are any NaN values in the column\n",
    "        if shoreline_df_annual[column].isnull().any():\n",
    "            # Calculate the median value of the column (excluding NaN values)\n",
    "            median_value = shoreline_df_annual[column].median()\n",
    "        \n",
    "            # Replace NaN values with the calculated median value\n",
    "            shoreline_df_annual[column].fillna(median_value, inplace=True)\n",
    "            \n",
    "            # Final check and fill/drop remaining NaNs\n",
    "            shoreline_df_annual.fillna(method='ffill', inplace=True)  # Forward fill\n",
    "            shoreline_df_annual.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "            # Optional: Drop any rows that still have NaNs\n",
    "            shoreline_df_annual.dropna(inplace=True)\n",
    "\n",
    "            # Ensure no NaNs are left before model training\n",
    "            if shoreline_df_annual.isna().any().any():\n",
    "                print(f\"NaNs remain in shorelines data for {name}\")\n",
    "                continue  # Skip this iteration if NaNs are still present\n",
    "    \n",
    "    # Add the DataFrame to the dictionary with site name as key\n",
    "    annual_data[name] = {\n",
    "        'waves': wave_df_annual,\n",
    "        'shorelines': shoreline_df_annual\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4021fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">pp1d_from_N</th>\n",
       "      <th colspan=\"3\" halign=\"left\">swh_from_N</th>\n",
       "      <th colspan=\"3\" halign=\"left\">pp1d_from_NNE</th>\n",
       "      <th>swh_from_NNE</th>\n",
       "      <th>...</th>\n",
       "      <th>pp1d_from_NW</th>\n",
       "      <th colspan=\"3\" halign=\"left\">swh_from_NW</th>\n",
       "      <th colspan=\"3\" halign=\"left\">pp1d_from_NNW</th>\n",
       "      <th colspan=\"3\" halign=\"left\">swh_from_NNW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>...</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "      <th>10th_quantile</th>\n",
       "      <th>50th_quantile</th>\n",
       "      <th>90th_quantile</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <th>months</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1984</th>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.568388</td>\n",
       "      <td>1.159329</td>\n",
       "      <td>2.113522</td>\n",
       "      <td>3.003425</td>\n",
       "      <td>10.858882</td>\n",
       "      <td>11.489670</td>\n",
       "      <td>12.711223</td>\n",
       "      <td>0.784305</td>\n",
       "      <td>1.325311</td>\n",
       "      <td>2.069451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.00150</td>\n",
       "      <td>11.651595</td>\n",
       "      <td>12.257721</td>\n",
       "      <td>0.581329</td>\n",
       "      <td>0.698613</td>\n",
       "      <td>0.849621</td>\n",
       "      <td>11.091750</td>\n",
       "      <td>12.218543</td>\n",
       "      <td>12.235737</td>\n",
       "      <td>0.602527</td>\n",
       "      <td>...</td>\n",
       "      <td>15.556421</td>\n",
       "      <td>0.944696</td>\n",
       "      <td>1.942769</td>\n",
       "      <td>2.496285</td>\n",
       "      <td>8.990899</td>\n",
       "      <td>10.270372</td>\n",
       "      <td>12.130041</td>\n",
       "      <td>0.648540</td>\n",
       "      <td>1.061916</td>\n",
       "      <td>1.478057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.03587</td>\n",
       "      <td>10.035870</td>\n",
       "      <td>10.035870</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>0.782094</td>\n",
       "      <td>10.085448</td>\n",
       "      <td>10.130714</td>\n",
       "      <td>10.176769</td>\n",
       "      <td>0.842339</td>\n",
       "      <td>...</td>\n",
       "      <td>16.012798</td>\n",
       "      <td>0.664541</td>\n",
       "      <td>1.757484</td>\n",
       "      <td>2.532742</td>\n",
       "      <td>10.067748</td>\n",
       "      <td>10.614656</td>\n",
       "      <td>11.018130</td>\n",
       "      <td>0.748528</td>\n",
       "      <td>0.768114</td>\n",
       "      <td>0.841217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.969103</td>\n",
       "      <td>0.695217</td>\n",
       "      <td>1.110330</td>\n",
       "      <td>1.613585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.222614</td>\n",
       "      <td>0.719590</td>\n",
       "      <td>1.001971</td>\n",
       "      <td>1.661999</td>\n",
       "      <td>7.460350</td>\n",
       "      <td>7.915628</td>\n",
       "      <td>11.252801</td>\n",
       "      <td>0.975197</td>\n",
       "      <td>1.129680</td>\n",
       "      <td>1.177368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022</th>\n",
       "      <th>8</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.344798</td>\n",
       "      <td>0.656596</td>\n",
       "      <td>0.987281</td>\n",
       "      <td>1.394260</td>\n",
       "      <td>8.006328</td>\n",
       "      <td>8.218338</td>\n",
       "      <td>8.557211</td>\n",
       "      <td>0.701709</td>\n",
       "      <td>0.809183</td>\n",
       "      <td>0.956542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.501251</td>\n",
       "      <td>1.084615</td>\n",
       "      <td>1.306514</td>\n",
       "      <td>1.547684</td>\n",
       "      <td>8.186714</td>\n",
       "      <td>8.565018</td>\n",
       "      <td>8.682043</td>\n",
       "      <td>1.401431</td>\n",
       "      <td>1.507832</td>\n",
       "      <td>1.520468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.812888</td>\n",
       "      <td>0.663309</td>\n",
       "      <td>1.016424</td>\n",
       "      <td>1.373330</td>\n",
       "      <td>6.105875</td>\n",
       "      <td>6.203791</td>\n",
       "      <td>8.034147</td>\n",
       "      <td>0.661129</td>\n",
       "      <td>0.741183</td>\n",
       "      <td>0.833177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.569991</td>\n",
       "      <td>1.552865</td>\n",
       "      <td>1.781020</td>\n",
       "      <td>2.520800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pp1d_from_N                                swh_from_N  \\\n",
       "             10th_quantile 50th_quantile 90th_quantile 10th_quantile   \n",
       "years months                                                           \n",
       "1984  1            0.00000      0.000000      0.000000      0.000000   \n",
       "      2           11.00150     11.651595     12.257721      0.581329   \n",
       "      3           10.03587     10.035870     10.035870      0.782094   \n",
       "      4            0.00000      0.000000      0.000000      0.000000   \n",
       "      5            0.00000      0.000000      0.000000      0.000000   \n",
       "...                    ...           ...           ...           ...   \n",
       "2022  8            0.00000      0.000000      0.000000      0.000000   \n",
       "      9            0.00000      0.000000      0.000000      0.000000   \n",
       "      10           0.00000      0.000000      0.000000      0.000000   \n",
       "      11           0.00000      0.000000      0.000000      0.000000   \n",
       "      12           0.00000      0.000000      0.000000      0.000000   \n",
       "\n",
       "                                         pp1d_from_NNE                \\\n",
       "             50th_quantile 90th_quantile 10th_quantile 50th_quantile   \n",
       "years months                                                           \n",
       "1984  1           0.000000      0.000000      0.000000      0.000000   \n",
       "      2           0.698613      0.849621     11.091750     12.218543   \n",
       "      3           0.782094      0.782094     10.085448     10.130714   \n",
       "      4           0.000000      0.000000      0.000000      0.000000   \n",
       "      5           0.000000      0.000000      0.000000      0.000000   \n",
       "...                    ...           ...           ...           ...   \n",
       "2022  8           0.000000      0.000000      0.000000      0.000000   \n",
       "      9           0.000000      0.000000      0.000000      0.000000   \n",
       "      10          0.000000      0.000000      0.000000      0.000000   \n",
       "      11          0.000000      0.000000      0.000000      0.000000   \n",
       "      12          0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "                            swh_from_NNE  ...  pp1d_from_NW   swh_from_NW  \\\n",
       "             90th_quantile 10th_quantile  ... 90th_quantile 10th_quantile   \n",
       "years months                              ...                               \n",
       "1984  1           0.000000      0.000000  ...     16.568388      1.159329   \n",
       "      2          12.235737      0.602527  ...     15.556421      0.944696   \n",
       "      3          10.176769      0.842339  ...     16.012798      0.664541   \n",
       "      4           0.000000      0.000000  ...     11.969103      0.695217   \n",
       "      5           0.000000      0.000000  ...     11.222614      0.719590   \n",
       "...                    ...           ...  ...           ...           ...   \n",
       "2022  8           0.000000      0.000000  ...     11.344798      0.656596   \n",
       "      9           0.000000      0.000000  ...     16.501251      1.084615   \n",
       "      10          0.000000      0.000000  ...     12.812888      0.663309   \n",
       "      11          0.000000      0.000000  ...     14.569991      1.552865   \n",
       "      12          0.000000      0.000000  ...      0.000000      0.000000   \n",
       "\n",
       "                                         pp1d_from_NNW                \\\n",
       "             50th_quantile 90th_quantile 10th_quantile 50th_quantile   \n",
       "years months                                                           \n",
       "1984  1           2.113522      3.003425     10.858882     11.489670   \n",
       "      2           1.942769      2.496285      8.990899     10.270372   \n",
       "      3           1.757484      2.532742     10.067748     10.614656   \n",
       "      4           1.110330      1.613585      0.000000      0.000000   \n",
       "      5           1.001971      1.661999      7.460350      7.915628   \n",
       "...                    ...           ...           ...           ...   \n",
       "2022  8           0.987281      1.394260      8.006328      8.218338   \n",
       "      9           1.306514      1.547684      8.186714      8.565018   \n",
       "      10          1.016424      1.373330      6.105875      6.203791   \n",
       "      11          1.781020      2.520800      0.000000      0.000000   \n",
       "      12          0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "                            swh_from_NNW                              \n",
       "             90th_quantile 10th_quantile 50th_quantile 90th_quantile  \n",
       "years months                                                          \n",
       "1984  1          12.711223      0.784305      1.325311      2.069451  \n",
       "      2          12.130041      0.648540      1.061916      1.478057  \n",
       "      3          11.018130      0.748528      0.768114      0.841217  \n",
       "      4           0.000000      0.000000      0.000000      0.000000  \n",
       "      5          11.252801      0.975197      1.129680      1.177368  \n",
       "...                    ...           ...           ...           ...  \n",
       "2022  8           8.557211      0.701709      0.809183      0.956542  \n",
       "      9           8.682043      1.401431      1.507832      1.520468  \n",
       "      10          8.034147      0.661129      0.741183      0.833177  \n",
       "      11          0.000000      0.000000      0.000000      0.000000  \n",
       "      12          0.000000      0.000000      0.000000      0.000000  \n",
       "\n",
       "[468 rows x 96 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave_df_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4649d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store standardized shorelines data\n",
    "standardized_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9008748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names and prepare datasets\n",
    "for name in site_names:\n",
    "    shoreline_df = annual_data[name]['shorelines']\n",
    "    reference_columns = shoreline_df.columns.difference(['site', 'years', 'months'])\n",
    "    standardized_columns = {col: f\"{name}_{col}\" for col in reference_columns}\n",
    "    shoreline_df.rename(columns=standardized_columns, inplace=True)\n",
    "    standardized_data[name] = shoreline_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a8066f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine standardized data into a single DataFrame\n",
    "combined_shorelines = pd.DataFrame(index=pd.MultiIndex.from_product(\n",
    "    [shoreline_df.index.get_level_values(0).unique(), shoreline_df.index.get_level_values(1).unique()],\n",
    "    names=['years', 'months']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4188c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from all sites\n",
    "for name in site_names:\n",
    "    df_to_merge = standardized_data[name]\n",
    "\n",
    "    # Drop 'years' and 'months' columns if they exist as regular columns\n",
    "    if 'years' in df_to_merge.columns:\n",
    "        df_to_merge = df_to_merge.drop('years', axis=1)\n",
    "    if 'months' in df_to_merge.columns:\n",
    "        df_to_merge = df_to_merge.drop('months', axis=1)\n",
    "\n",
    "    # Reset index if 'years' and 'months' are part of the index\n",
    "    if 'years' in df_to_merge.index.names and 'months' in df_to_merge.index.names:\n",
    "        df_to_merge = df_to_merge.reset_index()\n",
    "\n",
    "    combined_shorelines = combined_shorelines.merge(df_to_merge, \n",
    "                                                   on=['years', 'months'], \n",
    "                                                   how='left')\n",
    "\n",
    "# Handling NaNs\n",
    "for column in combined_shorelines.columns:\n",
    "    if column not in ['site', 'years', 'months']:\n",
    "        # Group by 'years' and fill NaNs with the mean of the respective year\n",
    "        combined_shorelines[column] = combined_shorelines.groupby('years')[column].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# Drop rows with NaN values if any remain\n",
    "combined_shorelines.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5661bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "X = combined_waves  # Assuming 'combined_waves' contains your features\n",
    "y = combined_shorelines  # Assuming 'combined_shorelines' contains your target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73aac701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e68c359",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1025640, 468]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting the data into training and testing sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2614\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2614\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2617\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2619\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:455\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    454\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 455\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1025640, 468]"
     ]
    }
   ],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2e58f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 51)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2bd7fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>years</th>\n",
       "      <th>months</th>\n",
       "      <th>pp1d_from_N</th>\n",
       "      <th>swh_from_N</th>\n",
       "      <th>pp1d_from_NNE</th>\n",
       "      <th>swh_from_NNE</th>\n",
       "      <th>pp1d_from_NE</th>\n",
       "      <th>swh_from_NE</th>\n",
       "      <th>pp1d_from_ENE</th>\n",
       "      <th>swh_from_ENE</th>\n",
       "      <th>...</th>\n",
       "      <th>swh_from_WSW</th>\n",
       "      <th>pp1d_from_W</th>\n",
       "      <th>swh_from_W</th>\n",
       "      <th>pp1d_from_WNW</th>\n",
       "      <th>swh_from_WNW</th>\n",
       "      <th>pp1d_from_NW</th>\n",
       "      <th>swh_from_NW</th>\n",
       "      <th>pp1d_from_NNW</th>\n",
       "      <th>swh_from_NNW</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <th>months</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1984</th>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.774342</td>\n",
       "      <td>1.137815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CCFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.765041</td>\n",
       "      <td>1.142238</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CCFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.759686</td>\n",
       "      <td>1.147293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CCFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.759122</td>\n",
       "      <td>1.152189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CCFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1984</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.761941</td>\n",
       "      <td>1.157718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CCFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2022</th>\n",
       "      <th>12</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.355039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MEIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.528793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MEIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.540640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MEIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.549170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MEIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.861999</td>\n",
       "      <td>2.558647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MEIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1025640 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              years  months  pp1d_from_N  swh_from_N  pp1d_from_NNE  \\\n",
       "years months                                                          \n",
       "1984  1        1984       1          0.0         0.0            0.0   \n",
       "      1        1984       1          0.0         0.0            0.0   \n",
       "      1        1984       1          0.0         0.0            0.0   \n",
       "      1        1984       1          0.0         0.0            0.0   \n",
       "      1        1984       1          0.0         0.0            0.0   \n",
       "...             ...     ...          ...         ...            ...   \n",
       "2022  12       2022      12          0.0         0.0            0.0   \n",
       "      12       2022      12          0.0         0.0            0.0   \n",
       "      12       2022      12          0.0         0.0            0.0   \n",
       "      12       2022      12          0.0         0.0            0.0   \n",
       "      12       2022      12          0.0         0.0            0.0   \n",
       "\n",
       "              swh_from_NNE  pp1d_from_NE  swh_from_NE  pp1d_from_ENE  \\\n",
       "years months                                                           \n",
       "1984  1                0.0           0.0          0.0            0.0   \n",
       "      1                0.0           0.0          0.0            0.0   \n",
       "      1                0.0           0.0          0.0            0.0   \n",
       "      1                0.0           0.0          0.0            0.0   \n",
       "      1                0.0           0.0          0.0            0.0   \n",
       "...                    ...           ...          ...            ...   \n",
       "2022  12               0.0           0.0          0.0            0.0   \n",
       "      12               0.0           0.0          0.0            0.0   \n",
       "      12               0.0           0.0          0.0            0.0   \n",
       "      12               0.0           0.0          0.0            0.0   \n",
       "      12               0.0           0.0          0.0            0.0   \n",
       "\n",
       "              swh_from_ENE  ...  swh_from_WSW  pp1d_from_W  swh_from_W  \\\n",
       "years months                ...                                          \n",
       "1984  1                0.0  ...      0.000000     0.000000    0.000000   \n",
       "      1                0.0  ...      0.000000     0.000000    0.000000   \n",
       "      1                0.0  ...      0.000000     0.000000    0.000000   \n",
       "      1                0.0  ...      0.000000     0.000000    0.000000   \n",
       "      1                0.0  ...      0.000000     0.000000    0.000000   \n",
       "...                    ...  ...           ...          ...         ...   \n",
       "2022  12               0.0  ...      2.355039     0.000000    0.000000   \n",
       "      12               0.0  ...      2.528793     0.000000    0.000000   \n",
       "      12               0.0  ...      2.540640     0.000000    0.000000   \n",
       "      12               0.0  ...      2.549170     0.000000    0.000000   \n",
       "      12               0.0  ...      0.000000    11.861999    2.558647   \n",
       "\n",
       "              pp1d_from_WNW  swh_from_WNW  pp1d_from_NW  swh_from_NW  \\\n",
       "years months                                                           \n",
       "1984  1           11.774342      1.137815      0.000000     0.000000   \n",
       "      1           11.765041      1.142238      0.000000     0.000000   \n",
       "      1           11.759686      1.147293      0.000000     0.000000   \n",
       "      1           11.759122      1.152189      0.000000     0.000000   \n",
       "      1            0.000000      0.000000     11.761941     1.157718   \n",
       "...                     ...           ...           ...          ...   \n",
       "2022  12           0.000000      0.000000      0.000000     0.000000   \n",
       "      12           0.000000      0.000000      0.000000     0.000000   \n",
       "      12           0.000000      0.000000      0.000000     0.000000   \n",
       "      12           0.000000      0.000000      0.000000     0.000000   \n",
       "      12           0.000000      0.000000      0.000000     0.000000   \n",
       "\n",
       "              pp1d_from_NNW  swh_from_NNW  site  \n",
       "years months                                     \n",
       "1984  1                 0.0           0.0  CCFT  \n",
       "      1                 0.0           0.0  CCFT  \n",
       "      1                 0.0           0.0  CCFT  \n",
       "      1                 0.0           0.0  CCFT  \n",
       "      1                 0.0           0.0  CCFT  \n",
       "...                     ...           ...   ...  \n",
       "2022  12                0.0           0.0  MEIA  \n",
       "      12                0.0           0.0  MEIA  \n",
       "      12                0.0           0.0  MEIA  \n",
       "      12                0.0           0.0  MEIA  \n",
       "      12                0.0           0.0  MEIA  \n",
       "\n",
       "[1025640 rows x 35 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d35b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51edd0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eceeae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a28af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace87a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94fd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f4fab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
